AssembleModel(
  (backbone): VisionTransformer(
    (patch_emb): Conv2d(3, 192, kernel_size=(17, 17), stride=(16, 16), padding=(8, 8))
    (transformer): Sequential(
      (0): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (1): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (2): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (3): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (4): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (5): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (6): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (7): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (8): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (9): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (10): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (11): TransformerEncoder(
        (pre_norm_mha): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): MultiHeadAttention(
            (qkv_proj): LinearLayer()
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (out_proj): LinearLayer()
            (softmax): Softmax(dim=-1)
          )
          (2): Dropout(p=0.0, inplace=False)
        )
        (pre_norm_ffn): Sequential(
          (0): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (1): LinearLayer()
          (2): SiLU()
          (3): Dropout(p=0.0, inplace=False)
          (4): LinearLayer()
          (5): Dropout(p=0.0, inplace=False)
        )
      )
      (12): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
    )
    (pos_embed): SinusoidalPositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (emb_dropout): Dropout(p=0.1, inplace=False)
  )
  (head): FC(
    (classifier): Linear(in_features=192, out_features=6, bias=True)
  )
)