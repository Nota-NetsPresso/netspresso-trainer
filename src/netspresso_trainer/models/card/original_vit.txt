VisionTransformer(
  (patch_emb): Conv2d(3, 192, kernel_size=(17, 17), stride=(16, 16), padding=(8, 8), bias=True)
  (transformer): Sequential(
    (0): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (1): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (2): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (3): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (4): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (5): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (6): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (7): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (8): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (9): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (10): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (11): TransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm)
    (12): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (classifier): LinearLayer(in_features=192, out_features=1000, bias=True, channel_first=False)
  (pos_embed): SinusoidalPositionalEncoding(dropout=0.1)
)