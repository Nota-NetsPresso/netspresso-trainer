model:
  task: detection
  name: efficientformer_l1
  checkpoint: ./weights/efficientformer/efficientformer_l1_1000d.safetensors
  fx_model_checkpoint: ~
  resume_optimizer_checkpoint: ~
  freeze_backbone: False
  architecture:
    full: ~ # auto
    backbone:
      name: efficientformer
      params:
        num_attention_heads: 8
        attention_hidden_size: 256  # attention_hidden_size_splitted * num_attention_heads
        attention_dropout_prob: 0.
        attention_ratio: 4
        intermediate_ratio: 4
        hidden_dropout_prob: 0.
        hidden_activation_type: 'gelu'
        drop_path_rate: 0.
        vit_num: 1
      stage_params:
        - 
          num_blocks: 3
          hidden_sizes: 48
        - 
          num_blocks: 2
          hidden_sizes: 96
        - 
          num_blocks: 6
          hidden_sizes: 224
        - 
          num_blocks: 4
          hidden_sizes: 448
    neck:
      name: fpn
      params:
        num_outs: 4
        start_level: 0
        end_level: -1
        add_extra_convs: False
        relu_before_extra_convs: False
    head:
      name: anchor_decoupled_head
      params:
        anchor_sizes: [[32,], [64,], [128,], [256,]]
        aspect_ratios: [0.5, 1.0, 2.0]
        num_layers: 1
        norm_layer: batch_norm
        # postprocessor - decode
        topk_candidates: 1000
        score_thresh: 0.05
        # postprocessor - nms
        nms_thresh: 0.45
        class_agnostic: False
  losses:
    - criterion: retinanet_loss
      weight: ~