model:
  task: detection
  name: efficientformer_l1
  checkpoint:
    use_pretrained: True
    load_head: False
    path: ~
    fx_model_path: ~
    optimizer_path: ~
  freeze_backbone: False
  architecture:
    full: ~ # auto
    backbone:
      name: efficientformer
      params:
        num_attention_heads: 8
        attention_channels: 256  # attention_hidden_size_splitted * num_attention_heads
        attention_dropout_prob: 0.
        attention_value_expansion_ratio: 4
        ffn_intermediate_ratio: 4
        ffn_dropout_prob: 0.
        ffn_act_type: 'gelu'
        vit_num: 1
      stage_params:
        - 
          num_blocks: 3
          channels: 48
        - 
          num_blocks: 2
          channels: 96
        - 
          num_blocks: 6
          channels: 224
        - 
          num_blocks: 4
          channels: 448
    neck:
      name: fpn
      params:
        num_outs: 4
        start_level: 0
        end_level: -1
        add_extra_convs: False
        relu_before_extra_convs: False
    head:
      name: anchor_decoupled_head
      params:
        anchor_sizes: [[32,], [64,], [128,], [256,]]
        aspect_ratios: [0.5, 1.0, 2.0]
        num_layers: 1
        norm_type: batch_norm
  postprocessor:
    topk_candidates: 1000
    score_thresh: 0.05  # decode
    nms_thresh: 0.45  # nms
    class_agnostic: False
  losses:
    - criterion: retinanet_loss
      weight: ~