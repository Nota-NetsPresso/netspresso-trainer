{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NetsPresso Trainer is a PyTorch training repository specialized for model training with torch.fx graphmodule conversion and automatic compression.  </p> <p>By using NetsPresso Trainer and NetsPresso together, you can take advantage of features such as model compression without significant performance drop, ONNX export, and NVIDIA TensorRT export.</p> <p>Enjoy training and deploying your own edge AI models with NetsPresso Trainer!</p>"},{"location":"benchmarks/benchmarks/","title":"Benchmarks","text":"<p>We are working on creating pretrained weights with NetsPresso Trainer and our own resources. We base training recipes on the official repositories or original papers to replicate the performance of models.</p> <p>For models that we have not yet trained with NetsPresso Trainer, we provide their pretrained weights from other awesome repositories. We have converted several models' weights into our own model architectures. Therefore, we denote models that provide weights obtained from other repositories by appending an \"*\" to the model's name in the benchmark performance table. If you are interested in the original models, refer to the remarks in the table. We appreciate all the original authors and we also do our best to make other values.</p> <p>If you have a better recipe, please share with us anytime. We appreciate all efforts to improve our models</p>"},{"location":"benchmarks/benchmarks/#classification","title":"Classification","text":"Dataset Model Weights Input shape Acc_1 Acc_5 Params FLOPs NetsPresso Remarks ImageNet1K EfficientFormer-l1* download (224, 224) 80.03 94.90 11.84M 2.60G Supported snap-research/EfficientFormer ImageNet1K MixNet-s download (224, 224) 75.08 92.32 4.13M 0.51G Supported - ImageNet1K MixNet-m download (224, 224) 76.37 93.07 5.01M 0.71G Supported - ImageNet1K MixNet-l download (224, 224) 80.78 95.23 7.33M 1.16G Supported - ImageNet1K MobileNetV3-small* download (224, 224) 67.67 87.41 2.54M 0.12G Supported torchvision ImageNet1K MobileNetV3-large* download (224, 224) 75.31 92.64 5.48M 0.45G Supported torchvision ImageNet1K MobileNetV4-conv-small* download (224, 224) 73.74 91.39 3.77M 0.38G Supported timm ImageNet1K MobileNetV4-conv-medium* download (224, 224) 79.09 94.67 9.72M 1.68G Supported timm ImageNet1K MobileNetV4-conv-large* download (256, 256) 81.84 95.74 32.59M 5.72G Supported timm ImageNet1K MobileNetV4-hybrid-medium* download (224, 224) 80.49 95.40 11.07M 1.96G Supported timm ImageNet1K MobileNetV4-hybrid-large* download (384, 384) 83.80 96.72 37.76M 15.54G Supported timm ImageNet1K MobileViT-s* download (256, 256) 78.21 94.13 5.58M 4.07G Supported No input z-norm, apple/ml-cvnets ImageNet1K ResNet18 download (224, 224) 69.11 88.87 11.69M 3.64G Supported - ImageNet1K ResNet34 download (224, 224) 72.42 90.87 21.80M 7.34G Supported - ImageNet1K ResNet50 download (224, 224) 79.67 94.82 25.56M 8.22G Supported - ImageNet1K ViT-tiny* download (224, 224) 72.90 91.17 5.70M 2.52G Supported No input z-norm, apple/ml-cvnets"},{"location":"benchmarks/benchmarks/#semantic-segmentation","title":"Semantic segmentation","text":"Dataset Model Weights Input shape mIoU Pixel acc Params FLOPs NetsPresso Remarks ADE20K SegFormer-b0* download (512, ?) or (?, 512) 37.15 76.78 3.75M 17.01G Supported mmsegmentation, Resize short edge to 512 Cityscapes PIDNet-s* download (1024, 2048) 78.76 96.12 7.72M 95.03G Supported XuJiacong/PIDNet"},{"location":"benchmarks/benchmarks/#object-detection","title":"Object detection","text":"Dataset Model Weights Input shape mAP50 mAP75 mAP50:95 Params FLOPs NetsPresso Remarks COCO-val RT-DETR_res18* download (640, 640) 65.77 52.75 48.49 20.18M 40.36G Supported No input z-norm, lyuwenyu/RT-DETR COCO-val RT-DETR_res50* download (640, 640) 72.64 59.50 54.73 42.94M 138.36G Supported No input z-norm, lyuwenyu/RT-DETR COCO-val yolov9-tiny download (640, 640) 50.03 38.63 36.02 2.44M 9.99G Supported No input z-norm COCO-val yolov9-s* download (640, 640) 62.63 51.13 47.13 7.23M 26.87G Supported No input z-norm, YOLO COCO-val yolov9-m* download (640, 640) 67.43 56.13 51.72 20.12M 77.08G Supported No input z-norm, YOLO COCO-val yolov9-c* download (640, 640) 69.16 57.90 53.28 25.50M 103.17G Supported No input z-norm, YOLO COCO-val YOLOX-nano* download (416, 416) 41.30 27.90 26.33 0.91M 1.08G Supported Megvii-BaseDetection/YOLOX, conf_thresh=0.01, nms_thresh=0.65 COCO-val YOLOX-tiny* download (416, 416) 50.69 36.18 34.00 5.06M 6.45G Supported Megvii-BaseDetection/YOLOX, conf_thresh=0.01, nms_thresh=0.65 COCO-val YOLOX-s download (640, 640) 58.56 44.10 40.63 8.97M 26.81G Supported conf_thresh=0.01, nms_thresh=0.65 COCO-val YOLOX-m* download (640, 640) 65.00 51.34 47.04 25.33M 73.76G Supported Megvii-BaseDetection/YOLOX, conf_thresh=0.01, nms_thresh=0.65 COCO-val YOLOX-l* download (640, 640) 68.07 55.18 50.68 54.21M 155.65G Supported Megvii-BaseDetection/YOLOX, conf_thresh=0.01, nms_thresh=0.65 COCO-val YOLOX-x* download (640, 640) 69.13 56.46 51.79 99.07M 281.94G Supported Megvii-BaseDetection/YOLOX, conf_thresh=0.01, nms_thresh=0.65 COCO-val YOLO-Fastest-v2 download (640, 640) 25.03 11.60 12.78 0.25M 0.74G Supported conf_thresh=0.01, nms_thresh=0.65"},{"location":"benchmarks/compression_compat_matrix/","title":"Compression compatibility matrix","text":"<p>The models in NetsPresso Trainer can be compressed with NetsPresso's compressor module.  This document provides a matrix showing the compatibility of the NetsPresso Trainer models with a NetsPresso compressor.</p> <p>Supported compression methods include pruning and filter decomposition. For more detailed information, please refer to NetsPresso.</p> Task Model Compressor Ramarks Classification EfficientFormer-l1 \u2705 Classification MixNet-s \u2705 Classification MobileNetV3-s \u2705 Classification MovileViT-s \u2705 Classification ResNet50 \u2705 Classification ViT-tiny \u2705 Segmentation PIDNet-s \u2705 Segmentation SegFormet-b0 \u2705 Detection YOLOX-s \u2705"},{"location":"benchmarks/device_matrix/drpai/","title":"DRP-AI","text":"<p>The models in NetsPresso Trainer can be converted to DRP-AI format by NetsPresso's Launcher module. The converted DRP-AI model's performance can be measured on actual boards using NetsPresso's Benchmarker module. For more detailed information, please refer to NetsPresso.</p> <p>Note that the latency value only measures the time for the model's computation and does not include the time on data preprocessing or postprocessing.</p>"},{"location":"benchmarks/device_matrix/drpai/#renesas-rz-v2l","title":"Renesas RZ V2L","text":""},{"location":"benchmarks/device_matrix/drpai/#fp16","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification ResNet18 (224, 224) 3 23.9425 - - onnx_opset=13 Classification ResNet34 (224, 224) 3 40.0319 - - onnx_opset=13 Classification ResNet50 (224, 224) 3 58.8135 - - onnx_opset=13 Detection YOLOX-s (640, 640) 4 196.642 - - onnx_opset=13 Detection YOLOX-m (640, 640) 4 412.707 - - onnx_opset=13 Detection YOLOX-l (640, 640) 4 687.293 - - onnx_opset=13"},{"location":"benchmarks/device_matrix/openvino/","title":"OpenVINO","text":"<p>The models in NetsPresso Trainer can be converted to OpenVINO format by NetsPresso's Launcher module. The converted OpenVINO model's performance can be measured on actual boards using NetsPresso's Benchmarker module. For more detailed information, please refer to NetsPresso.</p> <p>Note that the latency value only measures the time for the model's computation and does not include the time on data preprocessing or postprocessing.</p>"},{"location":"benchmarks/device_matrix/openvino/#intel-xeon-w-2233","title":"Intel Xeon W-2233","text":""},{"location":"benchmarks/device_matrix/openvino/#fp16","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 5.65 - - onnx_opset=13 Classification MixNet-s (224, 224) 3 3.97 - - onnx_opset=13 Classification MixNet-m (224, 224) 3 6.17 - - onnx_opset=13 Classification MixNet-l (224, 224) 3 8.13 - - onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 2.36 - - onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 4.42 - - onnx_opset=13 Classification MovileViT-s (256, 256) 3 11.69 - - onnx_opset=13 Classification ResNet18 (224, 224) 3 5.94 - - onnx_opset=13 Classification ResNet34 (224, 224) 3 11.2 - - onnx_opset=13 Classification ResNet50 (224, 224) 3 13.34 - - onnx_opset=13 Classification ViT-tiny (224, 224) 3 6.77 - - onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 17.95 - - onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 65.45 - - onnx_opset=13 Detection YOLOX-s (640, 640) 4 44.81 - - onnx_opset=13 Detection YOLOX-m (640, 640) 4 114.02 - - onnx_opset=13 Detection YOLOX-l (640, 640) 4 227.48 - - onnx_opset=13"},{"location":"benchmarks/device_matrix/tensorrt/","title":"TensorRT","text":"<p>The models in NetsPresso Trainer can be converted to TensorRT format by NetsPresso's Launcher module. During this process, you can specify the JetPack version. The converted TensorRT model's performance can be measured on actual boards using NetsPresso's Benchmarker module.</p> <p>Using NetsPresso, you can utilize more various JetPack versions and devices than listed in this document. For more detailed information, please refer to NetsPresso.</p> <p>Note that the latency value only measures the time for the model's computation and does not include the time on data preprocessing or postprocessing.</p>"},{"location":"benchmarks/device_matrix/tensorrt/#jetpack-60","title":"JetPack 6.0","text":""},{"location":"benchmarks/device_matrix/tensorrt/#jetson-orin-nano","title":"Jetson Orin Nano","text":""},{"location":"benchmarks/device_matrix/tensorrt/#fp16","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 4.45334 23.0 - onnx_opset=13 Classification MixNet-s (224, 224) 3 6.33559 12.0 1.0 onnx_opset=13 Classification MixNet-m (224, 224) 3 9.07441 16.0 1.0 onnx_opset=13 Classification MixNet-l (224, 224) 3 11.4235 23.0 1.0 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 1.56278 4.0 - onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 2.51209 11.0 - onnx_opset=13 Classification MovileViT-s (256, 256) 3 7.13308 18.0 - Classification ResNet18 (224, 224) 3 1.99792 23.0 - onnx_opset=13 Classification ResNet34 (224, 224) 3 3.48368 42.0 - onnx_opset=13 Classification ResNet50 (224, 224) 3 4.80201 48.0 - onnx_opset=13 Classification ViT-tiny (224, 224) 3 3.11549 11.0 - onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 7.32771 25.0 - onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 22.6924 69.0 - onnx_opset=13 Detection YOLOX-s (640, 640) 4 15.508 29.0 - onnx_opset=13 Detection YOLOX-m (640, 640) 4 32.276 25.0 - onnx_opset=13 Detection YOLOX-l (640, 640) 4 53.4317 69.0 - onnx_opset=13"},{"location":"benchmarks/device_matrix/tensorrt/#jetpack-501","title":"JetPack 5.0.1","text":""},{"location":"benchmarks/device_matrix/tensorrt/#jetson-agx-orin","title":"Jetson AGX Orin","text":""},{"location":"benchmarks/device_matrix/tensorrt/#fp16_1","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 1.35066 23.0 302.0 onnx_opset=13 Classification MixNet-s (224, 224) 3 2.1774 964.0 920.0 onnx_opset=13 Classification MixNet-m (224, 224) 3 2.83159 969.0 921.0 onnx_opset=13 Classification MixNet-l (224, 224) 3 3.30286 977.0 920.0 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 0.721624 4.0 302.0 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 0.919789 10.0 302.0 onnx_opset=13 Classification ResNet18 (224, 224) 3 0.520046 23.0 302.0 onnx_opset=13 Classification ResNet34 (224, 224) 3 0.87017 42.0 301.0 onnx_opset=13 Classification ResNet50 (224, 224) 3 1.1421 48.0 302.0 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 1.87514 855.0 836.0 onnx_opset=13 Detection YOLOX-s (640, 640) 4 3.18338 31.0 301.0 onnx_opset=13 Detection YOLOX-m (640, 640) 4 6.2141 68.0 302.0 onnx_opset=13 Detection YOLOX-l (640, 640) 4 9.27001 130.0 302.0 onnx_opset=13"},{"location":"benchmarks/device_matrix/tensorrt/#jetpack-46","title":"JetPack 4.6","text":""},{"location":"benchmarks/device_matrix/tensorrt/#jetson-nano","title":"Jetson Nano","text":""},{"location":"benchmarks/device_matrix/tensorrt/#fp16_2","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 20.3864 695.0 601.0 onnx_opset=13 Classification MixNet-s (224, 224) 3 22.5372 692.0 603.0 onnx_opset=13 Classification MixNet-m (224, 224) 3 32.5569 691.0 602.0 onnx_opset=13 Classification MixNet-l (224, 224) 3 42.1082 691.0 602.0 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 5.0241 692.0 603.0 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 11.278 692.0 600.0 onnx_opset=13 Classification ResNet18 (224, 224) 3 10.8578 693.0 601.0 onnx_opset=13 Classification ResNet34 (224, 224) 3 19.5193 691.0 602.0 onnx_opset=13 Classification ResNet50 (224, 224) 3 29.2816 690.0 600.0 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 36.4498 694.0 603.0 onnx_opset=13 Detection YOLOX-s (640, 640) 4 95.8883 693.0 600.0 onnx_opset=13 Detection YOLOX-m (640, 640) 4 224.517 692.0 602.0 onnx_opset=13 Detection YOLOX-l (640, 640) 4 415.363 691.0 602.0 onnx_opset=13"},{"location":"benchmarks/device_matrix/tensorrt/#jetson-nx","title":"Jetson NX","text":""},{"location":"benchmarks/device_matrix/tensorrt/#fp16_3","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 3.76573 893.0 886.0 onnx_opset=13 Classification MixNet-s (224, 224) 3 5.13436 894.0 888.0 onnx_opset=13 Classification MixNet-m (224, 224) 3 6.67466 893.0 888.0 onnx_opset=13 Classification MixNet-l (224, 224) 3 8.16476 891.0 886.0 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 1.24565 881.0 886.0 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 1.96175 893.0 887.0 onnx_opset=13 Classification ResNet18 (224, 224) 3 1.78444 893.0 887.0 onnx_opset=13 Classification ResNet34 (224, 224) 3 3.28956 887.0 886.0 onnx_opset=13 Classification ResNet50 (224, 224) 3 4.16903 893.0 887.0 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 6.80566 895.0 886.0 onnx_opset=13 Detection YOLOX-s (640, 640) 4 13.7659 892.0 887.0 onnx_opset=13 Detection YOLOX-m (640, 640) 4 29.2506 892.0 887.0 onnx_opset=13 Detection YOLOX-l (640, 640) 4 49.0844 896.0 888.0 onnx_opset=13"},{"location":"benchmarks/device_matrix/tensorrt/#jetson-tx2","title":"Jetson TX2","text":""},{"location":"benchmarks/device_matrix/tensorrt/#fp16_4","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 8.02968 727.0 657.0 onnx_opset=13 Classification MixNet-s (224, 224) 3 10.6572 720.0 657.0 onnx_opset=13 Classification MixNet-m (224, 224) 3 13.8955 721.0 657.0 onnx_opset=13 Classification MixNet-l (224, 224) 3 17.8804 721.0 657.0 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 3.64664 722.0 657.0 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 4.96121 721.0 656.0 onnx_opset=13 Classification ResNet18 (224, 224) 3 4.18611 720.0 656.0 onnx_opset=13 Classification ResNet34 (224, 224) 3 7.52092 722.0 657.0 onnx_opset=13 Classification ResNet50 (224, 224) 3 11.0185 718.0 656.0 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 14.3191 723.0 657.0 onnx_opset=13 Detection YOLOX-s (640, 640) 4 35.8665 723.0 657.0 onnx_opset=13 Detection YOLOX-m (640, 640) 4 82.2585 719.0 655.0 onnx_opset=13 Detection YOLOX-l (640, 640) 4 153.366 719.0 657.0 onnx_opset=13"},{"location":"benchmarks/device_matrix/tensorrt/#jetson-xavier","title":"Jetson Xavier","text":""},{"location":"benchmarks/device_matrix/tensorrt/#fp16_5","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 2.6756 890.0 887.0 onnx_opset=13 Classification MixNet-s (224, 224) 3 3.68271 891.0 887.0 onnx_opset=13 Classification MixNet-m (224, 224) 3 4.87919 876.0 888.0 onnx_opset=13 Classification MixNet-l (224, 224) 3 5.89479 895.0 886.0 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 1.00566 896.0 886.0 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 1.50451 886.0 887.0 onnx_opset=13 Classification ResNet18 (224, 224) 3 1.10724 890.0 887.0 onnx_opset=13 Classification ResNet34 (224, 224) 3 1.99773 891.0 886.0 onnx_opset=13 Classification ResNet50 (224, 224) 3 2.83026 892.0 888.0 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 4.56521 894.0 886.0 onnx_opset=13 Detection YOLOX-s (640, 640) 4 8.83621 891.0 888.0 onnx_opset=13 Detection YOLOX-m (640, 640) 4 19.1866 892.0 887.0 onnx_opset=13 Detection YOLOX-l (640, 640) 4 30.6859 894.0 886.0 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/","title":"TFLite","text":"<p>The models in NetsPresso Trainer can be converted to TFLite format by NetsPresso's Launcher module. The converted TFLite model's performance can be measured on actual boards using NetsPresso's Benchmarker module. For more detailed information, please refer to NetsPresso.</p> <p>Note that the latency value only measures the time for the model's computation and does not include the time on data preprocessing or postprocessing.</p>"},{"location":"benchmarks/device_matrix/tflite/#alif-ensemble-e7-devkit-gen-2","title":"Alif Ensemble E7 DevKit Gen 2","text":"<ul> <li>Cortex-M55 + Ethos-U55</li> <li>Ethosu delegates</li> </ul>"},{"location":"benchmarks/device_matrix/tflite/#int8","title":"INT8","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification MixNet-s (224, 224) 3 46.0708 - - onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 13.0257 - - onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#nxp-imx93","title":"NXP iMX93","text":"<ul> <li>Cortex-A55 + Ethos-U65</li> <li>Ethosu delegates</li> </ul>"},{"location":"benchmarks/device_matrix/tflite/#int8_1","title":"INT8","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 21.6117 - onnx_opset=13 Classification MixNet-s (224, 224) 3 20.1424 - onnx_opset=13 Classification MixNet-m (224, 224) 3 27.7012 - onnx_opset=13 Classification MixNet-l (224, 224) 3 37.3885 - onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 3.21802 - onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 7.05413 - onnx_opset=13 Classification MovileViT-s - - - - - Classification ResNet18 (224, 224) 3 22.8633 - onnx_opset=13 Classification ResNet34 - - - - - Classification ResNet50 (224, 224) 3 38.5456 - onnx_opset=13 Classification ViT-tiny (224, 224) 3 412.846 - onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 87.6305 - onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 2274.03 - onnx_opset=13 Detection YOLOX-s (640, 640) 4 133.775 - onnx_opset=13 Detection YOLOX-m (640, 640) 4 279.712 - onnx_opset=13 Detection YOLOX-l (640, 640) 4 503.778 - onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#raspberry-pi-5","title":"Raspberry Pi 5","text":""},{"location":"benchmarks/device_matrix/tflite/#fp16","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 164.574 - 115.844 onnx_opset=13 Classification MixNet-s (224, 224) 3 49.5337 - 38.4688 onnx_opset=13 Classification MixNet-m (224, 224) 3 78.0415 - 64.1562 onnx_opset=13 Classification MixNet-l (224, 224) 3 117.185 - 96.0156 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 4.077 - 20.6875 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 15.2487 - 55.2188 onnx_opset=13 Classification MovileViT-s (256, 256) 3 228.259 - 115.234 onnx_opset=13 Classification ResNet18 (224, 224) 3 55.2718 - 120.078 onnx_opset=13 Classification ResNet34 (224, 224) 3 98.0536 - 217.828 onnx_opset=13 Classification ResNet50 (224, 224) 3 130.835 - 278.656 onnx_opset=13 Classification ViT-tiny (224, 224) 3 305.55 - 56.0 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 133.98 - 119.578 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 1199.34 - 354.812 onnx_opset=13 Detection YOLOX-s (640, 640) 4 418.725 - 169.594 onnx_opset=13 Detection YOLOX-m (640, 640) 4 1176.87 - 357.891 onnx_opset=13 Detection YOLOX-l (640, 640) 4 2355.4 - 666.844 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#int8_2","title":"INT8","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 30.6753 - 21.2969 onnx_opset=13 Classification MixNet-s (224, 224) 3 80.8769 - 20.4375 onnx_opset=13 Classification MixNet-m (224, 224) 3 82.4275 - 32.3906 onnx_opset=13 Classification MixNet-l (224, 224) 3 73.1796 - 43.875 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 6.88714 - 3.53125 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 78.3648 - 11.5156 onnx_opset=13 Classification MovileViT-s (256, 256) 3 224.878 - 30.3281 onnx_opset=13 Classification ResNet18 - - - - - Classification ResNet34 - - - - - Classification ResNet50 - - - - - Classification ViT-tiny (224, 224) 3 49.8123 - 12.0469 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 39.093 - 36.1719 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 649.016 - 273.594 onnx_opset=13 Detection YOLOX-s (640, 640) 4 100.01 - 42.25 onnx_opset=13 Detection YOLOX-m (640, 640) 4 217.008 - 87.5 onnx_opset=13 Detection YOLOX-l (640, 640) 4 446.359 - 153.625 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#raspberry-pi-4b","title":"Raspberry Pi 4B","text":""},{"location":"benchmarks/device_matrix/tflite/#fp16_1","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 439.871 - 119.438 onnx_opset=13 Classification MixNet-s (224, 224) 3 108.501 - 40.3906 onnx_opset=13 Classification MixNet-m (224, 224) 3 148.488 - 65.3008 onnx_opset=13 Classification MixNet-l (224, 224) 3 231.303 - 98.332 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 14.1937 - 21.5352 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 43.9362 - 55.5117 onnx_opset=13 Classification MovileViT-s (256, 256) 3 417.108 - 117.504 onnx_opset=13 Classification ResNet18 (224, 224) 3 254.588 - 121.074 onnx_opset=13 Classification ResNet34 (224, 224) 3 486.278 - 220.734 onnx_opset=13 Classification ResNet50 (224, 224) 3 556.2 - 281.504 onnx_opset=13 Classification ViT-tiny (224, 224) 3 286.019 - 57.5938 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 605.416 - 122.723 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 2294.5 - 357.348 onnx_opset=13 Detection YOLOX-s (640, 640) 4 1488.71 - 171.43 onnx_opset=13 Detection YOLOX-m (640, 640) 4 4542.29 - 360.41 onnx_opset=13 Detection YOLOX-l (640, 640) 4 10087.7 - 669.797 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#int8_3","title":"INT8","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 80.4513 - 23.1289 onnx_opset=13 Classification MixNet-s (224, 224) 3 119.517 - 21.418 onnx_opset=13 Classification MixNet-m (224, 224) 3 211.811 - 34.3984 onnx_opset=13 Classification MixNet-l (224, 224) 3 276.174 - 45.75 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 18.4982 - 4.19531 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 57.0669 - 12.1953 onnx_opset=13 Classification MovileViT-s (256, 256) 3 287.328 - 32.2891 onnx_opset=13 Classification ResNet18 - - - - - Classification ResNet34 - - - - - Classification ResNet50 - - - - - Classification ViT-tiny (224, 224) 3 197.057 - 12.3945 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 227.03 - 38.4648 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 1393.21 - 275.422 onnx_opset=13 Detection YOLOX-s (640, 640) 4 533.657 - 46.2852 onnx_opset=13 Detection YOLOX-m (640, 640) 4 1468.42 - 87.9766 onnx_opset=13 Detection YOLOX-l (640, 640) 4 3133.25 - 154.941 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#raspberry-pi-3b-plus","title":"Raspberry Pi 3B PLUS","text":""},{"location":"benchmarks/device_matrix/tflite/#fp16_2","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 1414.44 - 119.656 onnx_opset=13 Classification MixNet-s (224, 224) 3 227.58 - 40.5938 onnx_opset=13 Classification MixNet-m (224, 224) 3 348.734 - 65.5273 onnx_opset=13 Classification MixNet-l (224, 224) 3 564.76 - 98.2148 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 28.9851 - 21.3633 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 114.964 - 55.293 onnx_opset=13 Classification MovileViT-s (256, 256) 3 906.407 - 117.707 onnx_opset=13 Classification ResNet18 (224, 224) 3 473.501 - 121.031 onnx_opset=13 Classification ResNet34 (224, 224) 3 864.672 - 220.539 onnx_opset=13 Classification ResNet50 (224, 224) 3 1091.6 - 281.406 onnx_opset=13 Classification ViT-tiny (224, 224) 3 841.007 - 57.5156 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 1139.91 - 122.859 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 5178.05 - 350.0 onnx_opset=13 Detection YOLOX-s (640, 640) 4 2881.02 - 171.043 onnx_opset=13 Detection YOLOX-m (640, 640) 4 7420.01 - 360.473 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#int8_4","title":"INT8","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 156.684 - 23.0898 onnx_opset=13 Classification MixNet-s (224, 224) 3 125.0 - 14.2891 onnx_opset=13 Classification MixNet-m (224, 224) 3 224.736 - 23.75 onnx_opset=13 Classification MixNet-l (224, 224) 3 281.604 - 32.0352 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 48.3053 - 3.98438 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 79.9677 - 12.1953 onnx_opset=13 Classification MovileViT-s (256, 256) 3 463.851 - 32.3008 onnx_opset=13 Classification ResNet18 - - - - - Classification ResNet34 - - - - - Classification ResNet50 - - - - - Classification ViT-tiny (224, 224) 3 307.119 - 12.3984 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 406.89 - 38.4805 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 2811.66 - 275.512 onnx_opset=13 Detection YOLOX-s (640, 640) 4 939.852 - 46.3281 onnx_opset=13 Detection YOLOX-m (640, 640) 4 2771.37 - 88.0156 onnx_opset=13 Detection YOLOX-l (640, 640) 4 5675.15 - 154.891 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#raspberry-pi-zero-2-w","title":"Raspberry Pi Zero 2 W","text":""},{"location":"benchmarks/device_matrix/tflite/#fp16_3","title":"FP16","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 1163.35 - 114.922 onnx_opset=13 Classification MixNet-s (224, 224) 3 225.104 - 39.8477 onnx_opset=13 Classification MixNet-m (224, 224) 3 315.645 - 63.7773 onnx_opset=13 Classification MixNet-l (224, 224) 3 448.629 - 95.4648 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 32.5132 - 23.1523 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 94.6444 - 56.9414 onnx_opset=13 Classification MovileViT-s (256, 256) 3 869.398 - 116.02 onnx_opset=13 Classification ResNet18 (224, 224) 3 441.127 - 120.402 onnx_opset=13 Classification ResNet34 (224, 224) 3 817.117 - 185.582 onnx_opset=13 Classification ResNet50 (224, 224) 3 940.785 - 233.328 onnx_opset=13 Classification ViT-tiny (224, 224) 3 713.305 - 58.3203 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 942.47 - 116.637 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 5779.5 - 264.441 onnx_opset=13 Detection YOLOX-s (640, 640) 4 2515.49 - 148.547 onnx_opset=13"},{"location":"benchmarks/device_matrix/tflite/#int8_5","title":"INT8","text":"Task Model Input shape Classes Latency (ms) GPU Memory (MB) CPU Memory (MB) Ramarks Classification EfficientFormer-l1 (224, 224) 3 248.033 - 23.6562 onnx_opset=13 Classification MixNet-s (224, 224) 3 152.92 - 13.2109 onnx_opset=13 Classification MixNet-m (224, 224) 3 257.34 - 20.4414 onnx_opset=13 Classification MixNet-l (224, 224) 3 340.322 - 28.0742 onnx_opset=13 Classification MobileNetV3-small (224, 224) 3 78.6897 - 5.73438 onnx_opset=13 Classification MobileNetV3-large (224, 224) 3 132.282 - 13.7188 onnx_opset=13 Classification MovileViT-s (256, 256) 3 701.015 - 30.9727 onnx_opset=13 Classification ResNet18 - - - - - Classification ResNet34 - - - - - Classification ResNet50 - - - - - Classification ViT-tiny (224, 224) 3 463.293 - 13.2422 onnx_opset=13 Segmentation PIDNet-s (512, 512) 35 574.138 - 32.5078 onnx_opset=13 Segmentation SegFormet-b0 (512, 512) 35 4373.37 - 148.633 onnx_opset=13 Detection YOLOX-s (640, 640) 4 1337.21 - 40.3945 onnx_opset=13 Detection YOLOX-m (640, 640) 4 3442.01 - 79.5859 onnx_opset=13 Detection YOLOX-l (640, 640) 4 7061.52 - 143.648 onnx_opset=13"},{"location":"benchmarks/tflite_runtime_examples/yolox-s/","title":"YOLOX-s","text":"<p>We provide TFLite runtime code to run models on devices, which can be found in the <code>tools/device_runtime</code> directory. This document presents an example of deploying and running the object detection model, YOLOX-s.</p>"},{"location":"benchmarks/tflite_runtime_examples/yolox-s/#install-miniconda-optional","title":"Install Miniconda (Optional)","text":"<p>Install Miniconda from Miniforge.</p> <pre><code>wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh\nbash Miniforge3-Linux-aarch64.sh # Agree and install\n</code></pre> <p>After initiate miniconda, create virtual environment for tflite runtime.</p> <pre><code>conda create -n tflite_runtime python=3.8.16\nconda activate tflite_runtime\n</code></pre>"},{"location":"benchmarks/tflite_runtime_examples/yolox-s/#ready-runtime-code","title":"Ready runtime code","text":"<p>To utilize provided TFLite runtime code, you should clone our repository into your device.</p> <pre><code>git clone -b master https://github.com/Nota-NetsPresso/netspresso-trainer.git\ncd netspresso_trainer/tools/device_runtime\n</code></pre>"},{"location":"benchmarks/tflite_runtime_examples/yolox-s/#install-packages","title":"Install packages","text":"<p>Install python packages with <code>requirements-tflite.txt</code>.</p> <pre><code>pip install -r requirements-tflite.txt\n</code></pre>"},{"location":"benchmarks/tflite_runtime_examples/yolox-s/#set-runtime-configuration","title":"Set runtime configuration","text":"<p>We have implemented the runtime code to accept a config file as input. The runtime configuration contains information about the model as well as details for preprocessing and postprocessing. For consistency, the structure of the preprocessing configuration aligns with the training augmentation configuration, and the postprocessor configuration structure aligns with the model configuration. Therefore, to run the YOLOX-s model with TFLite, you need to prepare the YAML configuration as following.</p> <pre><code>runtime:\n  task: detection\n  model_path: ./yolox_s.tflite\n  preprocess:\n    - \n      name: resize\n      size: 640\n      interpolation: bilinear\n      max_size: null\n      resize_criteria: long\n    - \n      name: pad\n      size: 640\n      fill: 114\n  postprocess:\n    score_thresh: 0.4\n    nms_thresh: 0.65\n</code></pre> Field  Description <code>task</code> (str) The type of task the model is designed for. We only support \"detection\" now. <code>model_path</code> (str) TFLite model tath to run. <code>preprocess</code> (list) The preprocessing pipeline to be applied to the input image. Refer to Augmentation page for more details. <code>postprocess</code> (dict) The postprocessing hyper parameters to be applied to the outputs. This field can be different according to the model. Refer to Model page for more details."},{"location":"benchmarks/tflite_runtime_examples/yolox-s/#run-with-camera-input","title":"Run with camera input","text":"<p>Execute the tflite_run.py Python file to run the YOLOX-s model on your device. Ensure that a camera is connected to the device, as the input images will be captured from the camera. When the file is executed, the process will read images from the connected camera, process them, and use the model to predict bounding boxes. These bounding boxes will be drawn using the OpenCV library and displayed in real-time in a window.</p> <pre><code>python tflite_run.py --config-path ./config/yolox-s-tflite.yaml\n</code></pre>"},{"location":"benchmarks/tflite_runtime_examples/yolox-s/#run-with-local-dataset","title":"Run with local dataset","text":"<p>To be updated ...</p>"},{"location":"components/data/","title":"Data","text":"<p>NetsPresso Trainer supports learning functions for various vision tasks with your custom data.  In addition to data stored in a local repository, it also supports learning with data accessible through APIs such as Hugging Face datasets.  Currently, the dataset formats supported by NetsPresso Trainer are fixed in a specific form, but we plan to expand to more dataset formats such as COCO format in the future.  </p> <p>On this page, we will guide you on the data format you need to learn with your custom data and how to learn using Hugging Face datasets. </p>"},{"location":"components/data/#local-custom-datasets","title":"Local custom datasets","text":""},{"location":"components/data/#supporting-image-formats","title":"Supporting image formats","text":"<p>For image data, various extension images are supported, but we recommend one of <code>.jpg</code>, <code>.jpeg</code>, <code>.png</code>, and <code>.bmp</code>. In this case, label data used in semantic segmentation must be saved as <code>.png</code> to prevent data loss and utilize image header information.  The following sections introduce how to organize data for each task. </p>"},{"location":"components/data/#common-configuration","title":"Common configuration","text":"<p>Regardless of the task, dataset directory should be organized as follows:</p> <ul> <li>Train: This directory should contain all the training images and corresponding label files.</li> <li>Validation: This directory should house validation images and their corresponding labels, used to tune the hyperparameters.</li> <li>Test: This directory should include test images and labels for final model evaluation.</li> </ul> <p>This structure should be reflected in your configuration file under the respective paths.</p> Field  Description <code>data.name</code> (str) The name of dataset. <code>data.task</code> (str) <code>classification</code> for image classification, <code>segmentation</code> for semantic segmentation, and <code>detection</code> for object detection. <code>data.format</code> <code>local</code> as an identifier of dataset format. <code>data.path.root</code> (str) Root directory of dataset. <code>data.path.train.image</code> (str) The directory for training images. Should be relative path to root directory. <code>data.path.valid.image</code> (str) The directory for validation images. Should be relative path to root directory. <code>data.path.test.image</code> (str) The directory for test images. Should be relative path to root directory."},{"location":"components/data/#image-classification","title":"Image classification","text":"<p>To train an image classification model using NetsPresso Trainer, users must organize their data according to a specified format.</p> <ul> <li>train images must be in same directory.</li> <li>validation images must be in same directory.</li> <li>labels for images are given by csv file. The csv file contains image file name and correspoinding class label.</li> </ul> Field  Description <code>data.id_mapping</code> (list) Class list for each class index. <code>data.path.train.label</code> (str) For classificaiton, label field must be path of <code>.csv</code> file. This should be relative path to root directory. <code>data.path.valid.label</code> (str) For classificaiton, label field must be path of <code>.csv</code> file. This should be relative path to root directory. <code>data.path.test.label</code> (str) For classificaiton, label field must be path of <code>.csv</code> file. This should be relative path to root directory. Data hierarchy example - ImageNet1K <pre><code>data/imagenet1k\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10026.JPEG\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10027.JPEG\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10029.JPEG\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 valid\n\u2502       \u251c\u2500\u2500 ILSVRC2012_val_00000001.JPEG\n\u2502       \u251c\u2500\u2500 ILSVRC2012_val_00000002.JPEG\n\u2502       \u251c\u2500\u2500 ILSVRC2012_val_00000003.JPEG\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 imagenet_train.csv\n    \u2514\u2500\u2500 imagenet_valid.csv\n</code></pre> Label csv example - ImageNet1K <pre><code>| image_id             | class    |\n|----------------------|----------|\n| n03792972_3671.JPEG  | 728      |\n| n04357314_4256.JPEG  | 810      |\n| n02965783_127.JPEG   | 576      |\n| n04465501_16825.JPEG | 289      |\n| n09246464_5059.JPEG  | 359      |\n| ... | ... |\n</code></pre> Data configuration example - ImageNet1K <pre><code>data:\n  name: imagenet1k\n  task: classification\n  format: local # local, huggingface\n  path:\n    root: path_to/IMAGENET1K # dataset root\n    train:\n      image: images/train # directory for training images\n      label: labels/imagenet_train.csv  # label for training images\n    valid:\n      image: images/valid  # directory for valid images\n      label: labels/imagenet_valid.csv  # label for valid images\n    test:\n      image: ~  # directory for test images\n      label: ~  # label for test images\n  id_mapping: [\"kit fox\", \"English setter\", \"Siberian husky\", \"Australian terrier\", ...]\n</code></pre>"},{"location":"components/data/#semantic-segmentation","title":"Semantic segmentation","text":"<p>To train a semantic segmentation model using NetsPresso Trainer, the data must be in the following formats: </p> <ul> <li>For each training image, there must be a label file (image) indicating the original image and the class index of each pixel of the image.</li> <li>Users must create an image and label directory under the root directory and put the corresponding files in each directory.</li> <li>In this case, training data and validation data can be distinguished in different directories. For example, training data can be placed in train/image, train/label directories, and validation data can be placed in valid/image, valid/label directories.</li> <li>Users must know the class name corresponding to each pixel value (RGB or L (grayscale) format) in the label file.</li> </ul> Field  Description <code>data.label_image_mode</code> (str) Image mode to convert the label. Should be one of <code>RGB</code>, <code>L</code>, and <code>P</code>. This field is not case-sensitive. <code>data.path.train.label</code> (str) For segmentation, label field must be path of label directory. This should be relative path to root directory. <code>data.path.valid.label</code> (str) For segmentation, label field must be path of label directory. This should be relative path to root directory. <code>data.path.test.label</code> (str) For segmentation, label field must be path of label directory. This should be relative path to root directory. <code>data.id_mapping</code> (dict, list) Key-value pair between label value (<code>RGB</code>, <code>L</code>, or <code>P</code>) and class name. Should be a dict of {label_value: classname} or a list of class names whose indices are same with the label value (image_mode: <code>L</code> or <code>P</code>). <code>data.palette</code> (dict) Color mapping for visualization. If <code>none</code>, automatically select the color for each class. Data hierarchy example - PascalVOC 2012 <pre><code>data/voc2012_seg\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 2007_000032.jpg\n\u2502   \u2502   \u251c\u2500\u2500 2007_000039.jpg\n\u2502   \u2502   \u251c\u2500\u2500 2007_000063.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 valid\n\u2502       \u251c\u2500\u2500 2007_000033.jpg\n\u2502       \u251c\u2500\u2500 2007_000042.jpg\n\u2502       \u251c\u2500\u2500 2007_000061.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 2007_000032.png\n    \u2502   \u251c\u2500\u2500 2007_000039.png\n    \u2502   \u251c\u2500\u2500 2007_000063.png\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 valid\n        \u251c\u2500\u2500 2007_000033.png\n        \u251c\u2500\u2500 2007_000042.png\n        \u251c\u2500\u2500 2007_000061.png\n        \u2514\u2500\u2500 ...\n</code></pre> Data configuration example - PascalVOC 2012 <pre><code>data:\n  name: voc2012\n  task: segmentation\n  format: local\n  path:\n    root: path_to/VOC12Dataset\n    train:\n      image: image/train\n      label: mask/train\n    valid:\n      image: image/valid\n      label: mask/valid\n    test:\n      image: ~  # directory for test images\n      label: ~  # directory for test labels\n    pattern:\n      image: ~\n      label: ~\n  label_image_mode: RGB\n  id_mapping:\n    (0, 0, 0): background\n    (128, 0, 0): aeroplane\n    (0, 128, 0): bicycle\n    (128, 128, 0): bird\n    (0, 0, 128): boat\n    (128, 0, 128): bottle\n    (0, 128, 128): bus\n    (128, 128, 128): car\n    (64, 0, 0): cat\n    (192, 0, 0): chair\n    (64, 128, 0): cow\n    (192, 128, 0): diningtable\n    (64, 0, 128): dog\n    (192, 0, 128): horse\n    (64, 128, 128): motorbike\n    (192, 128, 128): person\n    (0, 64, 0): pottedplant\n    (128, 64, 0): sheep\n    (0, 192, 0): sofa\n    (128, 192, 0): train\n    (0, 64, 128): tvmonitor\n    (128, 64, 128): void\n  pallete: ~\n</code></pre>"},{"location":"components/data/#object-detection","title":"Object detection","text":"<p>To train an object detection model using NetsPresso Trainer, the data must be in the following formats: </p> <ul> <li>For object detection model training, each training image must have a corresponding <code>.txt</code> file indicating the original image and the bounding box and class index corresponding to each bounding box of the image.</li> <li>The format of the bounding box follows the YOLO dataset format <code>[x_center, y_center, width, height]</code> (normalized).</li> <li>Each <code>.txt</code> file must contain one line for each bounding box.</li> <li>In this case, training data and validation data can be distinguished in different directories. For example, training data can be placed in train/image, train/label directories, and validation data can be placed in valid/image, valid/label directories.</li> <li>Users must know the class name corresponding to each class index in the label file.</li> </ul> Field  Description <code>data.path.train.label</code> (str) For detection, label field must be path of label directory. This should be relative path to root directory. <code>data.path.valid.label</code> (str) For detection, label field must be path of label directory. This should be relative path to root directory. <code>data.path.test.label</code> (str) For detection, label field must be path of label directory. This should be relative path to root directory. <code>data.id_mapping</code> (list) Class list for each class index. <code>data.palette</code> (dict) Color mapping for visualization. If <code>none</code>, automatically select the color for each class. Data hierarchy example - COCO 2017 <pre><code>data/coco2017\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 000000000009.jpg\n\u2502   \u2502   \u251c\u2500\u2500 000000000025.jpg\n\u2502   \u2502   \u251c\u2500\u2500 000000000030.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 valid\n\u2502       \u251c\u2500\u2500 000000000139.jpg\n\u2502       \u251c\u2500\u2500 000000000285.jpg\n\u2502       \u251c\u2500\u2500 000000000632.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 000000000009.txt\n    \u2502   \u251c\u2500\u2500 000000000025.txt\n    \u2502   \u251c\u2500\u2500 000000000030.txt\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 valid\n        \u251c\u2500\u2500 000000000139.txt\n        \u251c\u2500\u2500 000000000285.txt\n        \u251c\u2500\u2500 000000000632.txt\n        \u2514\u2500\u2500 ...\n</code></pre> Label txt example - COCO 2017 <pre><code>58 0.389578125 0.4161032863849765 0.038593749999999996 0.16314553990610328\n62 0.127640625 0.5051525821596244 0.23331249999999998 0.22269953051643193\n62 0.9341953125 0.583462441314554 0.127109375 0.18481220657276995\n56 0.60465625 0.6325469483568076 0.0875 0.24138497652582158\n56 0.5025078125 0.6273239436619719 0.096609375 0.2311737089201878\n56 0.6691953125 0.6189906103286384 0.047140625000000005 0.19098591549295774\n56 0.512796875 0.5282511737089202 0.03371875 0.02720657276995305\n0 0.6864453125 0.5319600938967136 0.082890625 0.3239671361502347\n0 0.612484375 0.4461971830985916 0.023625 0.08389671361502347\n68 0.811859375 0.5017253521126761 0.02303125 0.037488262910798126\n72 0.7863203125 0.5363732394366197 0.031703125 0.2542488262910798\n73 0.9561562499999999 0.7717018779342724 0.02240625 0.10730046948356808\n73 0.96825 0.7780751173708921 0.020125 0.10901408450704225\n74 0.7105546875 0.31 0.021828125 0.05136150234741784\n75 0.8865624999999999 0.8316079812206573 0.0573125 0.2104929577464789\n75 0.5569453125 0.5167018779342724 0.017765625 0.05293427230046949\n56 0.6516640625 0.5288262910798122 0.015046875000000001 0.029389671361502348\n75 0.388046875 0.4784154929577465 0.022218750000000002 0.04138497652582159\n75 0.5338359375 0.48794600938967136 0.015203125000000001 0.039272300469483566\n60 0.599984375 0.6471478873239437 0.19618750000000001 0.20875586854460096\n</code></pre> Custom object detection dataset example - COCO 2017 <pre><code>data:\n  name: coco2017\n  task: detection\n  format: local # local, huggingface\n  path:\n    root: ./data/coco2017 # dataset root\n    train:\n      image: images/train # directory for training images\n      label: labels/train # directory for training labels\n    valid:\n      image: images/valid # directory for valid images\n      label: labels/valid # directory for valid labels\n    test:\n      image: ~\n      label: ~\n    pattern:\n      image: ~\n      label: ~\n  id_mapping: ['person', 'bicycle', 'car', ...]\n  pallete: ~\n</code></pre>"},{"location":"components/data/#hugging-face-datasets","title":"Hugging Face datasets","text":"<p>NetsPresso Trainer is striving to support various dataset hubs and platforms.  As part of that effort and first step, NetsPresso Trainer can be used with data in Hugging Face datasets. </p> Field  Description <code>data.name</code> (str) The name of dataset. <code>data.task</code> (str) <code>classification</code> for image classification, <code>segmentation</code> for semantic segmentation, and <code>detection</code> for object detection. <code>data.format</code> <code>huggingface</code> as an identifier of dataset format. <code>data.metadata.custom_cache_dir</code> (str) Cache directory to load and save dataset files from Hugging Face. <code>data.metadata.repo</code> (str) Repository name. (e.g. <code>competitions/aiornot</code> represents the dataset <code>huggingface.co/datasets/competitions/aiornot</code>.) <code>data.metadata.subset</code> (str, optional) Subset name if the dataset contains multiple versions. <code>data.metadata.features.image</code> (str) The key representing the image at the dataset header. <code>data.metadata.features.label</code> (str) The key representing the label at the dataset header. Huggingface dataset example - beans <pre><code>data:\n  name: beans\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: beans\n    subset: ~\n    features:\n      image: image\n      label: labels\n</code></pre>"},{"location":"components/environment/","title":"Environment","text":"<p>The environment configuration contains values that determine the training environment, such as the number of workers for multi-processing and the GPU ids to be used. The following yaml is the environment configuration example.</p> <pre><code>environment: \n  seed: 1\n  batch_size: 8\n  num_workers: 4 \n  gpus: 0, 1, 2, 3\n</code></pre>"},{"location":"components/environment/#field-list","title":"Field list","text":"Field  Description <code>environment.seed</code> (int) Random seed. <code>environment.batch_size</code> (int) The number of samples in single batch input. <code>environment.num_workers</code> (int) The number of multi-processing workers to be used by the data loader. <code>environment.gpus</code> (str) GPU ids to use, this should be separated by commas."},{"location":"components/logging/","title":"Logging","text":"<p>NetsPresso Trainer provides training results in a variety of multiple formats. As a following example, users can determine most of output formats through boolean flags, and can adjust the intervals of evaluations and checkpoint saves with a simple configuration.</p> <pre><code>logging:\n  project_id: ~\n  output_dir: ./outputs\n  tensorboard: true\n  mlflow: true\n  image: true\n  stdout: true\n  model_save_options:\n    save_optimizer_state: true\n    save_best_only: false\n    best_model_criterion: loss # metric\n    sample_input_size: [512, 512] # Used for flops and onnx export\n    onnx_export_opset: 13 # Recommend in range [13, 17]\n    validation_epoch: &amp;validation_epoch 10\n    save_checkpoint_epoch: *validation_epoch  # Multiplier of `validation_epoch`.\n  metrics:\n    classwise_analysis: False\n    metric_names: ~ # None for default settings\n</code></pre>"},{"location":"components/logging/#tensorboard","title":"Tensorboard","text":"<p>We provide basic tensorboard to track your training status. Run the tensorboard with the following command: </p> <pre><code>tensorboard --logdir ./outputs --port 50001 --bind_all\n</code></pre> <p>Note that the default directory of saving result will be <code>./outputs</code> directory. The port number <code>50001</code> is same with the port forwarded in example docker setup. You can change with any port number available in your environment.</p>"},{"location":"components/logging/#mlflow","title":"MLflow","text":""},{"location":"components/logging/#what-is-mlflow","title":"What is MLflow?","text":"<p>MLflow is an open-source platform designed to streamline and optimize the machine learning (ML) lifecycle. It helps ML practitioners and teams efficiently manage experiments, track model performance, and ensure reproducibility across different environments. Whether you're working locally or deploying models on cloud platforms like AWS, Azure, or Databricks, MLflow provides the necessary tools to simplify ML operations. \ud83d\udd17 Official MLflow GitHub Repository</p>"},{"location":"components/logging/#how-to-install-mlflow","title":"How to Install MLflow","text":"<p>Getting started with MLflow is easy! Install it using pip: <pre><code>pip install mlflow\n</code></pre></p>"},{"location":"components/logging/#how-to-run-the-mlflow-tracking-server-anywhere","title":"How to Run the MLflow Tracking Server Anywhere","text":"<p>MLflow is highly flexible and can be deployed in multiple environments, including:</p> <ul> <li>\u2705 Local development</li> <li>\u2705 Amazon SageMaker</li> <li>\u2705 Amazon EC2</li> <li>\u2705 Azure ML</li> <li>\u2705 Databricks</li> </ul> <p>For a step-by-step setup guide, visit the official MLflow Documentation.</p> <p>\ud83d\udca1 Why Use MLflow?</p> <ul> <li>\ud83d\udcca Experiment Tracking \u2013 Log, compare, and visualize ML runs effortlessly.</li> <li>\ud83d\udd04 Model Management \u2013 Register, version, and deploy models seamlessly.</li> <li>\ud83d\udd17 Platform Agnostic \u2013 Works with any ML framework (TensorFlow, PyTorch, Scikit-Learn, etc.).</li> </ul>"},{"location":"components/logging/#how-to-log-to-the-mlflow-tracking-server","title":"How to Log to the MLflow tracking server","text":"<p>To connect to your MLflow tracking server, set the required environment variables: <pre><code>export MLFLOW_TRACKING_URI=&lt;your/mlflow/tracking/server/uri&gt;\nexport MLFLOW_EXPERIMENT_NAME=&lt;your/experiment/name&gt;  # default: `Default`\n</code></pre> Then, enable MLflow logging by updating your logging.yaml file.</p>"},{"location":"components/logging/#field-list","title":"Field list","text":"Field  Description <code>logging.project_id</code> (str) Project name to save the experiment. If <code>None</code>, it is set as <code>{task}_{model}</code> (e.g. <code>segmentation_segformer</code>). <code>logging.output_dir</code> (str) Root directory for saving the experiment. Default location is <code>./outputs</code>. <code>logging.tensorboard</code> (bool) Whether to use the tensorboard. <code>logging.mlflow</code> (bool) Whether to use the mlflow. <code>logging.image</code> (bool) Whether to save the validation results. It is ignored if the task is <code>classification</code>. <code>logging.stdout</code> (bool) Whether to log the standard output. <code>logging.model_save_options.save_optimizer_state</code> (bool) Whether to save optimizer state with model checkpoint to resume training. <code>logging.model_save_options.save_best_only</code> (bool) Whether to only the best model. <code>logging.model_save_options.best_model_criterion</code> (str) Criterion to determine which checkpoint is considered the best. One of 'loss' or 'metric'. <code>logging.model_save_options.sample_input_size</code> (list[int]) The size of the sample input used for calculating FLOPs and exporting the model to ONNX format. <code>logging.model_save_options.onnx_export_opset</code> (int) The ONNX opset version to be used for model export <code>logging.model_save_options.validation_epoch</code> (int) Validation frequency in total training process. <code>logging.model_save_options.save_checkpoint_epoch</code> (int) Checkpoint saving frequency in total training process. <code>logging.metrics.classwise_analysis</code> (bool) Whether to perform class-wise analysis of metrics during validation. <code>logging.metrics.metric_names</code> (list(str), optional) List of metric names to be logged. If not specified, default metrics for the task will be used."},{"location":"components/overview/","title":"Overview","text":"<p>NetsPresso Trainer and NetsPresso service provide a convenient experience in training, compressing, retraining, and deploying models for user devices, seamlessly. In that process, NetsPresso Trainer manages both training and retraining phases, ensuring the models are fully compatible with NetsPresso.</p> <p>NetsPresso Trainer categorizes essential parameters for training into six configuration modules. Each module is responsible for the following aspects:</p> <ul> <li>Data: Defines the structure of the user-customized or Hugging Face datasets for interpretation by NetsPresso Trainer.</li> <li>Augmentation: Defines the data augmentation recipe.</li> <li>Model: Defines the model architecture, postprocessor modules, loss modules, and pretrained weights.</li> <li>Training: Defines necessary elements like optimizer, epochs, and batch size for training.</li> <li>Logging: Defines output formats of training results.</li> <li>Environment: Defines the training environment, including GPU usage and dataloader multi-processing.</li> </ul> <p>This component section describes in detail the six configuration modules which are necessary to use NetsPresso Trainer. You can see yaml configuration examples in our public repository.</p>"},{"location":"components/overview/#advantage-of-netspresso-trainer","title":"Advantage of NetsPresso Trainer","text":""},{"location":"components/overview/#use-sota-models-fully-compatible-with-netspresso","title":"Use SOTA models fully compatible with NetsPresso","text":"<p>NetsPresso Trainer provides reimplemented SOTA models that ensure compatibility with NetsPresso. This allows users to avoid expending resources on changing model formats for model compression and device deployment. Therefore, the users can easily utilize SOTA models to their applications.</p>"},{"location":"components/overview/#easily-trainable-with-yaml-configuration","title":"Easily trainable with yaml configuration","text":"<p>NetsPresso Trainer encapsulates all the necessary values within configurations for model training. This enables extensive optimization attempts with mere modifications of these configuration files. Also, this enhances usability by using same configuration format for retraining compressed models.</p>"},{"location":"components/augmentation/overview/","title":"Augmentation - Overview","text":"<p>NetsPresso Trainer provides data augmentation functions to improve model performance, allowing users to configure their own training recipes as desired.  Data augmentation in NetsPresso Trainer is based on torch and torchvision, and all augmentations are implemented based on <code>pillow</code> images.</p> <p>In NetsPresso Trainer, users can create their desired augmentation recipe by composing a configuration as below. We separately define sample transform procedures for training and inference. </p> <p>Functions specified in the <code>train</code> and <code>inference</code> fields are applied sequentially as listed. Note that after all image processing is completed, the final image size must match with the size specified in <code>augmentation.img_size</code>.</p> <pre><code>augmentation:\n  img_size: &amp;img_size 256\n  train:\n    - \n      name: randomresizedcrop\n      size: *img_size\n      scale: [0.08, 1.0]\n      ratio: [0.75, 1.33]\n      interpolation: bilinear\n    - \n      name: randomhorizontalflip\n      p: 0.5\n    -\n      name: mixing\n      mixup: [0.25, 1.0]\n      cutmix: ~\n      inplace: false\n  inference:\n    - \n      name: resize\n      size: [*img_size, *img_size]\n      interpolation: bilinear\n      max_size: ~\n</code></pre>"},{"location":"components/augmentation/overview/#gradio-demo-for-simulating-the-transform","title":"Gradio demo for simulating the transform","text":"<p>In many learning function repositories, it is recommended to read the code and documentation or actually run the training to check the logs to see how augmentations are performed.  NetsPresso Trainer supports augmentation simulation to help users easily understand the augmentation recipe they have configured.  By copying and pasting the augmentation configuration into the simulator, users can preview how a specific image will be augmented in advance. However, transforms (e.g. Normalize, ToTensor) used to convert the image array for learning purposes are excluded from the simulation visualization process.  In particular, since this simulator directly imports the augmentation modules used in NetsPresso Trainer, users can use the same functions as the augmentation functions used in actual training to verify the results.  </p> <p>Our team hopes that the learning process with NetsPresso Trainer will become a more enjoyable experience for all users. </p>"},{"location":"components/augmentation/overview/#running-on-your-environment","title":"Running on your environment","text":"<p>Please run the gradio demo with following command:</p> <pre><code>bash scripts/run_simulator_augmentation.sh\n</code></pre>"},{"location":"components/augmentation/overview/#field-list","title":"Field list","text":"Field  Description <code>augmentation.img_size</code> (int) The image size of model input after finishing the data augmentation <code>augmentation.train</code> list[dict] List of transform functions for training. Augmentation process is defined on list order. <code>augmentation.inference</code> (list[dict]) List of transform functions for inference. Augmentation process is defined on list order."},{"location":"components/augmentation/transforms/","title":"Transforms","text":"<p>Users can easily create their own augmentation recipe simply by listing their desired data transform modules. It's possible to adjust the intensity and frequency of each transform module, and the listed transform modules are applied in sequence to produce augmented data. In NetsPresso Trainer, a visualization tool is also provided through a Gradio demo, allowing users to see how their custom augmentation recipe produces the data for the model.</p>"},{"location":"components/augmentation/transforms/#supporting-transforms","title":"Supporting transforms","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/augmentation/transforms/#centercrop","title":"CenterCrop","text":"<p>This augmentation follows the CenterCrop in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"centercrop\" to use <code>CenterCrop</code> transform. <code>size</code> (int or list) Desired output size of the crop. If size is an int, a square crop (size, size) is made. If provided a list of length 1, it will be interpreted as (size[0], size[0]). If a list of length 2 is provided, a square crop (size[0], size[1]) is made. CenterCrop example <pre><code>augmentation:\n  train:\n    - \n      name: centercrop\n      size: 224\n</code></pre>"},{"location":"components/augmentation/transforms/#colorjitter","title":"ColorJitter","text":"<p>This augmentation follows the ColorJitter in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"colorjitter\" to use <code>ColorJitter</code> transform. <code>brightness</code> (float or list) The brightness scale value is randomly selected within [max(0, 1 - brightness), 1 + brightness] or given [min, max] range. <code>contrast</code> (float or list) The contrast scale value is randomly selected within [max(0, 1 - contrast), 1 + contrast] or given [min, max] range. <code>saturation</code> (float or list) The saturation scale value is randomly selected within [max(0, 1 - saturation), 1 + saturation] or given [min, max] range. <code>hue</code> (float or list) The hue scale value is randomly selected within [max(0, 1 - hue), 1 + hue] or given [min, max] range. <code>p</code> (float) The probability of applying the color jitter. If set to <code>1.0</code>, the color transform is always applied. ColorJitter example <pre><code>augmentation:\n  train:\n    - \n      name: colorjitter\n      brightness: 0.25\n      contrast: 0.25\n      saturation: 0.25\n      hue: 0.1\n      p: 1.0\n</code></pre>"},{"location":"components/augmentation/transforms/#hsvjitter","title":"HSVJitter","text":"<p>HSVJitter is based on <code>augment_hsv</code> function of YOLOX repository. This transform convert input image to HSV format, and randomly adjust according to magnitude configuration. Each channel can be randomly adjusted or remain unchanged for every transform step.</p> Field  Description <code>name</code> (str) Name must be \"hsvjitter\" to use <code>HSVJitter</code> transform. <code>h_mag</code> (int) Randomly adjust the H channel within the range of [-h_mag, h_mag]. <code>s_mag</code> (int) Randomly adjust the S channel within the range of [-s_mag, s_mag]. <code>v_mag</code> (int) Randomly adjust the V channel within the range of [-v_mag, v_mag]. HSVJitter example <pre><code>augmentation:\n  train:\n    -\n      name: hsvjitter\n      h_mag: 5\n      s_mag: 30\n      v_mag: 30\n</code></pre>"},{"location":"components/augmentation/transforms/#mixing","title":"Mixing","text":"<p>We defined Mixing transform as the combination of CutMix and MixUp augmentation. This shuffles samples within a batch instead of processing per image. Therefore, Mixing transform must be in the last function of augmentation racipe if user wants to use it. Also, Mixing not assumes a batch size 1. If both MixUp and CutMix are activated, only one of two is randomly selected and used per batch processing.</p> <p>Cutmix augmentation is based on CutMix: Regularization strategy to train strong classifiers with localizable features and MixUp augmentation is based on mixup: Beyond empirical risk minimization. These implementation follow the RandomCutmix and RandomMixup in the ml-cvnets library.</p> <p>Currently, NetsPresso Trainer does not support a Gradio demo visualization for Mixing. This feature is planned to be added soon.</p> Field  Description <code>name</code> (str) Name must be \"mixing\" to use <code>Mixing</code> transform. <code>mixup</code> (list[float], optional) List of length 2 which contains [mixup alpha, applying probability]. If None, mixup is not applied. <code>cutmix</code> (list[float], optional) List of length 2 which contains [cutmix alpha, applying probability]. If None, cutmix is not applied. <code>inplace</code> (bool) Whether to operate as inplace. Mixing example <pre><code>augmentation:\n  train:\n    -\n      name: mixing\n      mixup: [0.25, 1.0]\n      cutmix: ~\n      inplace: false\n</code></pre>"},{"location":"components/augmentation/transforms/#mosaicdetection","title":"MosaicDetection","text":"<p>This MosaicDetection augmentation is based on YOLOX repository. For each sample, the following steps are taken to create an augmented sample.</p> <ul> <li>Load three additional images.</li> <li>Resize the four images to fit the <code>size</code>.</li> <li>Merge the four images into one, with the merge center point randomly determined.</li> <li>Apply a random affine transformation to the merged image. And resize output image to fit the <code>size</code>.</li> <li>Finally, if <code>enable_mixup</code> is set to <code>True</code>, apply a mixup transformation with a fixed alpha of 0.5. The mixup image also is randomly loaded from dataset.</li> </ul> Field  Description <code>name</code> (str) Name must be \"mosaicdetection\" to use <code>MosaicDetection</code> transform. <code>size</code> (list) Desired output size of the <code>MosaicDetection</code>. <code>mosaic_prob</code> (float) The probability of applying the <code>MosaicDetection</code>. If set to 1.0, it is always applied. <code>affine_scale</code> (list) Generate affine matrix with a scale range of [affine_scale[0], affine_scale[1]]. <code>degrees</code> (float) Generate affine matrix with a rotation range of [-degrees, degrees]. <code>translate</code> (float) Generate affine matrix with a translate range of [-translate, translate]. <code>shear</code> (float) Generate affine matrix with a shear range of [-shear, shear]. Randomly generate for each x-axis and y-axis. <code>enable_mixup</code> (bool) Whether to apply mixup. <code>mixup_prob</code> (float) The probability of applying the mixup. If set to 1.0, it is always applied. <code>mixup_scale</code> (list) Resize scale range for mixup image. <code>fill</code> (int)  This is used to fill pixels with constant value. <code>mosaic_off_duration</code> (int) Number of epochs for which the <code>MosaicDetection</code> transform is disabled at the end of training. MosaicDetection example <pre><code>augmentation:\n  train:\n    -\n      name: mosaicdetection\n      size: [*img_size, *img_size]\n      mosaic_prob: 1.0\n      affine_scale: [0.5, 1.5]\n      degrees: 10.0\n      translate: 0.1\n      shear: 2.0\n      enable_mixup: True\n      mixup_prob: 1.0\n      mixup_scale: [0.5, 1.5]\n      fill: 114\n      mosaic_off_duration: 10\n</code></pre>"},{"location":"components/augmentation/transforms/#nomalize","title":"Nomalize","text":"<p>Apply z-normalization to an image.</p> Field  Description <code>name</code> (str) Name must be \"normalize\" to use <code>Normalize</code> transform. <code>mean</code> (list[float]) The mean values for normalizing the image. <code>std</code> (list[float]) The standard deviation values for normalizing the image. Normalize example <pre><code>augmentation:\n  train:\n    -\n      name: normalize\n      mean: [0.485, 0.456, 0.406]\n      std: [0.229, 0.224, 0.225]\n</code></pre>"},{"location":"components/augmentation/transforms/#pad","title":"Pad","text":"<p>Pad an image with constant. This augmentation is based on the Pad in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"pad\" to use <code>Pad</code> transform. <code>size</code> (int or list) Padding on each border. If a single int is provided, target size is (<code>size</code>, <code>size</code>). If a list is provided, it must be length 2, and will produce size of (<code>size[0]</code>, <code>size[1]</code>) padded image. If each edge of input image is greater or equal than target size, padding will be not processed. <code>fill</code> (int or list) If a single int is provided this is used to fill pixels with constant value. If a list of length 3, it is used to fill R, G, B channels respectively. Pad example - 1 <pre><code>augmentation:\n  train:\n    -\n      name: pad\n      size: 512\n      fill: 0\n</code></pre> Pad example - 2 <pre><code>augmentation:\n  train:\n    -\n      name: pad\n      size: [512, 512]\n      fill: 0\n</code></pre>"},{"location":"components/augmentation/transforms/#posetopdownaffine","title":"PoseTopDownAffine","text":"<p>Apply affine transform based on given bounding box. This augmentation is based on the RandomBBoxTransform and TopDownAffine in mmpose library.</p> Field  Description <code>name</code> (str) Name must be \"pad\" to use <code>Pad</code> transform. <code>scale</code> (list) Randomly adjust box scale in range of [<code>scale[0]</code>, <code>scale[1]</code>] <code>scale_prob</code> (float) The probability of applying scaling. If set to <code>1.0</code>, scale of box always randomly adjusted. <code>translate</code> (float) Randomly adjust the offset of the box by adding translate factor * box size. The translate factor is random value in range of [<code>0</code>, <code>translate</code>]. <code>translate_prob</code> (float) The probability of applying translate. If set to <code>1.0</code>, offset of box always randomly adjusted. <code>rotation</code> (int) Random rotation range of affine transform. The random value determined in [<code>-rotation</code>, <code>rotation</code>]. This rotation angle is degree. <code>rotation_prob</code> (float) The probability of applying rotation. If set to <code>1.0</code>, affine transform matrix always contains rotation value. PoseTopDownAffine example <pre><code>augmentation:\n  train:\n    - \n      name: posetopdownaffine\n      scale: [0.75, 1.25]\n      scale_prob: 1.\n      translate: 0.1\n      translate_prob: 1.\n      rotation: 60\n      rotation_prob: 1.\n      size: [*img_size, *img_size]\n</code></pre>"},{"location":"components/augmentation/transforms/#randomcrop","title":"RandomCrop","text":"<p>Crop the given image at a random location. This augmentation follows the RandomCrop in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomcrop\" to use <code>RandomCrop</code> transform. <code>size</code> (int or list) Desired output size of the crop. If size is an int, a square crop (size, size) is made. If provided a list of length 1, it will be interpreted as (size[0], size[0]). If a list of length 2 is provided, a square crop (size[0], size[1]) is made. <code>fill</code> (int or list) If a single int is provided this is used to fill pixels with constant value. If a list of length 3, it is used to fill R, G, B channels respectively. RandomCrop example <pre><code>augmentation:\n  train:\n    - \n      name: randomcrop\n      size: 256\n      fill: 114\n</code></pre>"},{"location":"components/augmentation/transforms/#randomerasing","title":"RandomErasing","text":"<p>Erase random area of given image. This augmentation follows the RandomErasing in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomerasing\" to use <code>RancomErasing</code> transform. <code>p</code> (float) The probability of applying random erasing. If <code>1.0</code>, it always applies. <code>scale</code> (list) Range of proportion of erased area against input image. <code>ratio</code> (list) Range of aspect ratio of erased area. <code>value</code> (int, optional) Erasing value. If <code>None</code>, erase image with random noise. <code>inplace</code> (bool) Whether to operate as inplace. RandomErasing example <pre><code>augmentation:\n  train:\n    - \n      name: randomerasing\n      p: 0.5\n      scale: [0.02, 0.33]\n      ratio: [0.3, 3.3]\n      value: 0\n      inplace: False\n</code></pre>"},{"location":"components/augmentation/transforms/#randomhorizontalflip","title":"RandomHorizontalFlip","text":"<p>Horizontally flip the given image randomly with a given probability. This augmentation follows the RandomHorizontalFlip in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomhorizontalflip\" to use <code>RandomHorizontalFlip</code> transform. <code>p</code> (float) the probability of applying horizontal flip. If <code>1.0</code>, it always applies the flip. RandomHorizontalFlip example <pre><code>augmentation:\n  train:\n    - \n      name: randomhorizontalflip\n      p: 0.5\n</code></pre>"},{"location":"components/augmentation/transforms/#randomresize","title":"RandomResize","text":"<p>RandomResize transforms the input image to a random size within a specified range. This random size range is determined by [<code>base_size[0]</code> - <code>stride</code> * <code>v</code>, <code>base_size[1]</code> + <code>stride</code> * <code>v</code>] with <code>stride</code> interval, where the value <code>v</code> is an integer within the range of [<code>-random_range</code>, <code>random_range</code>]. E.g. If <code>base_size = [256, 256]</code>, <code>stride = 32</code>, <code>random_range = 2</code>, possible output image sizes are <code>[[192, 192], [224, 224], [256, 256], [288, 288], [320, 320]]</code>.</p> <p>Since applying random resize to every image arises the difficulty of a batch handling, random resize should be applied on a per-batch basis. However, due to current implementation constraints, it's challenging to apply randomness at the batch level, so a single random size is determined per dataloader worker. The size managed by each worker changes to at the start of each epoch. Therefore, to fully benefit from RandomResize, the number of workers set by <code>environment.num_workers</code> needs to be sufficiently large.</p> Field  Description <code>name</code> (str) Name must be \"randomresize\" to use <code>RandomResize</code> transform. <code>base_size</code> (list) The base size of the output image after random resizing. The output size is determined based on <code>base_size</code>. <code>stride</code> (int) The interval at which the size variation occurs. <code>random_range</code> (int) The range for random size variation. The final size is determined in range of [<code>base_size[0]</code> - <code>stride</code> * <code>v</code>, <code>base_size[1]</code> + <code>stride</code> * <code>v</code>] with <code>stride</code> interval, where <code>v</code> is an integer within the range of [<code>-random_range</code>, <code>random_range</code>]. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. RandomResize <pre><code>augmentation:\n  train:\n    - \n      name: randomresize\n      base_size: [256, 256]\n      stride: 32\n      random_range: 4\n      interpolation: 'bilinear'\n</code></pre>"},{"location":"components/augmentation/transforms/#randomresize2","title":"RandomResize2","text":"<p>RandomResize2 transforms the input image by resizing it based on a randomly selected scaling factor within a specified range. Note that RandomResize2 preserves the original aspect ratio, result image size might be largely different with <code>base_size</code>.</p> <p>Applying random resize to every image arises the difficulty of a batch handling, and RandomResize2 does not support per-batch target size handling function. We recommend to use RandomResize2 with other trasnform method.</p> Field  Description <code>name</code> (str) Name must be \"randomresize\" to use <code>RandomResize</code> transform. <code>base_size</code> (list) The base (height, width) of the target image, which is the initial size before applying randomness. <code>random_range</code> (int) A range [min_factor, max_factor] within which the random scaling factor is selected. The input image will be resized by a combination of base_size and random factors, while maintaining the aspect ratio. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. RandomResize <pre><code>augmentation:\n  train:\n    - \n      name: randomresize\n      base_size: [512, 2048]\n      random_range: [0.5, 1.5]\n      interpolation: 'bilinear'\n</code></pre>"},{"location":"components/augmentation/transforms/#randomresizedcrop","title":"RandomResizedCrop","text":"<p>Crop a random portion of image with different aspect of ratio in width and height, and resize it to a given size. This augmentation follows the RandomResizedCrop in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomresizedcrop\" to use <code>RandomResizedCrop</code> transform. <code>size</code> (int or list) Desired output size of the crop. If size is an int, a square crop (<code>size</code>, <code>size</code>) is made. If provided a list of length 1, it will be interpreted as (<code>size[0]</code>, <code>size[0]</code>). If a list of length 2 is provided, a crop with size (<code>size[0]</code>, <code>size[1]</code>) is made. <code>scale</code> (float or list) Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image. <code>ratio</code> (float or list) lower and upper bounds for the random aspect ratio of the crop, before resizing. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. RandomResizedCrop <pre><code>augmentation:\n  train:\n    - \n      name: randomresizedcrop\n      size: 256\n      scale: [0.08, 1.0]\n      ratio: [0.75, 1.33]\n      interpolation: 'bilinear'\n</code></pre>"},{"location":"components/augmentation/transforms/#randomverticalflip","title":"RandomVerticalFlip","text":"<p>Vertically flip the given image randomly with a given probability. This augmentation follows the RandomVerticalFlip in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"randomverticalflip\" to use <code>RandomVerticalFlip</code> transform. <code>p</code> (float) the probability of applying vertical flip. If <code>1.0</code>, it always applies the flip. RandomVerticalFlip example <pre><code>augmentation:\n  train:\n    - \n      name: randomverticalflip\n      p: 0.5\n</code></pre>"},{"location":"components/augmentation/transforms/#resize","title":"Resize","text":"<p>Naively resize the input image to the given size. This augmentation follows the Resize in torchvision library.</p> Field  Description <code>name</code> (str) Name must be \"resize\" to use <code>Resize</code> transform. <code>size</code> (int or list) Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller or larger edge of the image will be matched to this number and keep aspect ratio. Determining match to smaller or larger edge is determined by <code>resize_criteria</code>. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. <code>max_size</code> (int, optional) The maximum allowed for the longer edge of the resized image: if the longer edge of the image exceeds <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller edge may be shorter than <code>size</code>. This is only supported if <code>size</code> is an int. <code>resize_criteria</code> (str, optional) This field only used when <code>size</code> is int. This determines which side (shorter or longer) to match with <code>size</code>, and only can have 'short' or 'long' or <code>None</code>. i.e, if <code>resize_criteria</code> is 'short' and height &gt; width, then image will be rescaled to (size * height / width, size). Resize example - 1 <pre><code>augmentation:\n  train:\n    - \n      name: resize\n      size: [256, 256]\n      interpolation: 'bilinear'\n      max_size: ~\n      resize_criteria: ~\n</code></pre> Resize example - 2 <pre><code>augmentation:\n  train:\n    - \n      name: resize\n      size: 256\n      interpolation: 'bilinear'\n      max_size: ~\n      resize_criteria: long\n</code></pre>"},{"location":"components/augmentation/transforms/#totensor","title":"ToTensor","text":"<p>The <code>ToTensor</code> transform converts data into a tensor that can be fed into a PyTorch model. The <code>pixel_range</code> parameter allows you to specify the range of pixel values that each image data point can have.</p> Field  Description <code>name</code> (str) Name must be \"totensor\" to use <code>ToTensor</code> transform. <code>pixel_range</code> (float) The range of pixel values that the image data will be normalized to. ToTensor example - 1 <pre><code>augmentation:\n  train:\n    - \n      name: totensor\n      pixel_range: 1.0\n</code></pre> ToTensor example - 2 <pre><code>augmentation:\n  train:\n    - \n      name: totensor\n      pixel_range: 255.0\n</code></pre>"},{"location":"components/augmentation/transforms/#trivialaugmentwide","title":"TrivialAugmentWide","text":"<p>TrivialAugment based on TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. This augmentation follows the TrivialAugmentWide in the torchvision library. Currently, this transform function does not support segmentation and detection data.</p> Field  Description <code>name</code> (str) Name must be \"trivialaugmentwide\" to use <code>TrivialAugmentWide</code> transform. <code>num_magnitude_bins</code> (int) The number of different magnitude values. <code>interpolation</code> (str) Desired interpolation type. Supporting interpolations are 'nearest', 'bilinear' and 'bicubic'. <code>fill</code> (list or int, optional) Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. TrivialAugmentWide example <pre><code>augmentation:\n  train:\n    - \n      name: trivialaugmentwide\n      num_magnitude_bins: 31\n      interpolation: 'bilinear'\n      fill: ~\n</code></pre>"},{"location":"components/model/losses/","title":"Losses","text":"<p>Loss modules are very important in the training of neural networks, because as they guide and shape the learning process by minimizing the loss. They can affect the speed of convergence during training and the overall robustness of the model. </p> <p>However, loss functions can vary depending on the task, especially in tasks like detection and segmentation, where specialized loss functions are required. Therefore, NetsPresso Trainer provides a predefined variety of loss modules, designed for flexible use across different tasks. Users can seamlessly apply the appropriate loss function to their desired task through simple configuration settings.</p>"},{"location":"components/model/losses/#supporting-loss-modules","title":"Supporting loss modules","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, hence most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/model/losses/#crossentropyloss","title":"CrossEntropyLoss","text":"<p>Cross entropy loss. This loss follows the CrossEntropyLoss in torch library.</p> Field  Description <code>criterion</code> (str) Criterion must be \"cross_entropy\" to use <code>CrossEntropyLoss</code>. <code>label_smoothing</code> (float) Specifies the amount of smoothing when computing the loss, where 0.0 means no smoothing. <code>weight</code> (float) Weight for this cross entropy loss. Cross entropy loss example <pre><code>model:\n  losses:\n    - criterion: cross_entropy\n      label_smoothing: 0.1\n      weight: ~\n</code></pre>"},{"location":"components/model/losses/#sigmoidfocalloss","title":"SigmoidFocalLoss","text":"<p>Focal loss based on Focal loss for dense object detections. This loss follows the sigmoid_focal_loss in the torch library.</p> Field  Description <code>criterion</code> (str) Criterion must be \"focal_loss\" to use <code>SigmoidFocalLoss</code>. <code>alpha</code> (float) Balancing parameter alpha for focal loss. <code>gamma</code> (float) Focusing parameter gamma for focal loss. <code>weight</code> (float) Weight for this focal loss. Focal loss example <pre><code>model:\n  losses:\n    - criterion: focal_loss\n      alpha: 0.25\n      gamma: 2.0\n      weight: ~\n</code></pre>"},{"location":"components/model/losses/#yoloxloss","title":"YOLOXLoss","text":"<p>Loss module for AnchorFreeDecoupledHead. This loss follows the YOLOX implementation.</p> Field  Description <code>criterion</code> (str) Criterion must be \"yolox_loss\" to use <code>YOLOXLoss</code>. <code>weight</code> (float) Weight for this YOLOX loss. <code>l1_activate_epoch</code> (int) Activate l1 loss at <code>l1_activate_epoch</code> epoch. YOLOX loss example <pre><code>model:\n  losses:\n    - criterion: yolox_loss\n      weight: ~\n      l1_activate_epoch: 1\n</code></pre>"},{"location":"components/model/losses/#retinanetloss","title":"RetinaNetLoss","text":"<p>Loss module for AnchorDecoupledHead. This loss follows torchvision implementation, it contains classification loss via focal loss and box regression loss via L1 loss.</p> Field  Description <code>criterion</code> (str) Criterion must be \"retinanet_loss\" to use <code>RetinaNetLoss</code>. <code>weight</code> (float) Weight for this RetinaNet loss. RetinaNetLoss loss example <pre><code>model:\n  losses:\n    - criterion: retinanet_loss\n      weight: ~\n</code></pre>"},{"location":"components/model/losses/#pidnetloss","title":"PIDNetLoss","text":"<p>Loss module for PIDNet. This loss follows official implementation repository.</p> Field  Description <code>criterion</code> (str) Criterion must be \"pidnet_loss\" to use <code>PIDNetLoss</code>. <code>weight</code> (float) Weight for this PIDNet loss. <code>ignore_index</code> (int) A target value that is ignored and does not contribute to the input gradient. PIDNetLoss loss example <pre><code>model:\n  losses:\n    - criterion: pidnet_loss\n      weight: ~\n      ignore_index: 255\n</code></pre>"},{"location":"components/model/overview/","title":"Model - Overview","text":"<p>Netspresso Trainer provides a variety of backbones and heads, allowing flexible combinations. Users can choose appropriate backbones and heads based on their dataset and task requirements. The models can be optimized for on-device environments using Netspresso's compression and converting services.</p> <p>We provide a configuration format which can easily construct backbones and heads to meet user requirements. As composed in the example of the ResNet50 model below, backbones and heads are structured as separate fields, and then connected. Also, Users can freely choose suitable loss modules suitable for the head and task. The range of supported models and the detailed configuration definitions for each model are extensively described in the separated Models page.</p> <pre><code>model:\n  task: classification\n  name: resnet50\n  checkpoint:\n    use_pretrained: True\n    load_head: False\n    path: ~\n    optimizer_path: ~\n  freeze_backbone: False\n  architecture:\n    full: ...\n    backbone: ...\n    neck: ...\n    head: ...\n  postprocessor: ~\n  losses:\n    - criterion: cross_entropy\n      label_smoothing: 0.1\n      weight: ~\n</code></pre>"},{"location":"components/model/overview/#retraining-the-model-from-netspresso","title":"Retraining the model from NetsPresso","text":"<p>If you have compressed model from NetsPresso, then it's time to retrain your model to get the best performance. Netspresso Trainer uses the same configuration format for retraining torch.fx GraphModule. This can be executed by specifying the path to the torch.fx model in the <code>path</code>. The torch.fx model must have .pt extension which indicating it is a torch.fx model (In NetsPresso Trainer, vanilla torch model weights file has .safetensors extension). Since the torch.fx model file contains the complete model definition, fields like <code>architecture</code> become unnecessary, can be ignored.</p> <pre><code>model:\n  task: classification\n  name: resnet50\n  checkpoint:\n    use_pretrained: ~ # This field will be ignored\n    load_head: ~ # This field will be ignored\n    path: ./path_to_your_fx_model.pt # The torch.fx model must have .pt extension which indicating it is a torch.fx model\n    optimizer_path: ~ \n  freeze_backbone: False\n  architecture: ~ # This field will be ignored\n  postprocessor: ~\n  losses:\n    - criterion: cross_entropy\n      label_smoothing: 0.1\n      weight: ~\n</code></pre>"},{"location":"components/model/overview/#field-list","title":"Field list","text":"Field  Description <code>model.task</code> (str) We support \"classification\", \"segmentation\", and \"detection\" now. <code>model.name</code> (str) A nickname to identify the model. <code>model.checkpoint.use_pretrained</code> (bool) Whether to use the pretrained checkpoint. At first time, you will download and save the pretrained checkpoint. <code>model.checkpoint.load_head</code> (bool) Whether to use the pretrained checkpoint for <code>head</code> module. <code>model.checkpoint.path</code> (str) Checkpoint path to resume training. If <code>None</code> and <code>use_pretrained</code> is <code>False</code>, you can train you model from scratch. <code>model.checkpoint.optimizer_path</code> (str) Optimizer checkpoint path for resuming training. <code>model.freeze_backbone</code> (bool) Whether to freeze backbone in training. <code>model.architecture</code> (dict) Detailed configuration of the model architecture. Please see Model page to find NetsPresso supporting models. <code>model.postprocessor</code> (dict) Detailed configuration of the model postprocessor. Please see Postprocessor page <code>model.losses</code> (list) List of losses that model to learn. Please see Losses page to find NetsPresso supporting loss modules."},{"location":"components/model/postprocessors/","title":"Postprocessors","text":"<p>The postprocessor module is an essential component, designed to handle the output from deep learning models and apply necessary transformations to produce meaningful results. This module is particularly crucial for tasks such as object detection, where raw model outputs need to be processed into interpretable bounding boxes, confidence scores.</p> <p>We currently provide the postprocessor in a model-wise rigid format. This will be improved in the future to allow for more flexible usage.</p>"},{"location":"components/model/postprocessors/#supporting-postprocessors","title":"Supporting postprocessors","text":"<p>The current postprocessor is automatically determined based on the task name and model name. Users can utilize it by filling in the necessary hyperparameters under the params field of the postprocessor.</p>"},{"location":"components/model/postprocessors/#classification","title":"Classification","text":"<p>For classification, we don't have any postprocessor settings yet.</p> <pre><code>postprocessor: ~\n</code></pre>"},{"location":"components/model/postprocessors/#segmentation","title":"Segmentation","text":"<p>For segmentation, we don't have any postprocessor settings yet.</p> <pre><code>postprocessor: ~\n</code></pre>"},{"location":"components/model/postprocessors/#detection","title":"Detection","text":""},{"location":"components/model/postprocessors/#yolox","title":"YOLOX","text":"<p>YOLOX performs box decoding and NMS (Non-Maximum Suppression) on its output. The necessary hyperparameters for these processes are set as follows:</p> <pre><code>postprocessor: \n  params: \n    # postprocessor - decode\n    score_thresh: 0.01\n    # postprocessor - nms\n    nms_thresh: 0.65\n    class_agnostic: False\n</code></pre>"},{"location":"components/model/postprocessors/#yolofastestv2","title":"YOLOFastestV2","text":"<p>YOLOFastestV2 performs box decoding and NMS (Non-Maximum-Suppression) on its output predictions. The necessary hyperparameters for these processes are set as follows:</p> <pre><code>postprocessor:\n  params:\n    # postprocessor - decode\n    score_thresh: 0.01\n    # postprocessor - nms\n    nms_thresh: 0.65\n    anchors:\n      &amp;anchors\n      - [12.,18., 37.,49., 52.,132.]  # P2\n      - [115.,73., 119.,199., 242.,238.]  # P3\n    class_agnostic: False\n</code></pre>"},{"location":"components/model/postprocessors/#rt-detr","title":"RT-DETR","text":"<p>RT-DETR exclusively performs box decoding operations on its output predictions, distinguishing itself through its NMS-free design. Meanwhile, bipartite matching during training ensures one-to-one predictions, eliminating the need for non-maximum suppression (NMS) in the postprocessing stage. The necessary hyperparameters for the process are set as follows:</p> <pre><code>postprocessor:\n  params:\n    num_top_queries: 300\n    score_thresh: 0.01\n</code></pre>"},{"location":"components/training/ema/","title":"EMA (Exponential Moving Average)","text":"<p>In many cases, providing a model with averaged parameters brings performance benefits. The Exponential Moving Average (EMA) model is updated after each batch training step according to the following:</p> <pre><code>ema_param = decay * ema_param + (1. - decay) * training_model_param\n</code></pre> <p>If EMA is enabled, both validation and model saving are processed with EMA model. Note that after the validation phase, the training model parameters are reverted back to the non-averaged model.</p>"},{"location":"components/training/ema/#ema-decay-schedulers","title":"EMA decay schedulers","text":"<p>It is often benefits to start with a smaller decay value at the beginning of training and gradually use higher values as progresses. To this, we support some decay scheduling methods.</p>"},{"location":"components/training/ema/#constant-decay","title":"Constant decay","text":"<p>Constant decay keeps the decay value unchanged throughout the entire training process.</p> Field  Description <code>training.epochs</code> (str) Name must be \"constant_decay\" to use constant decay. <code>training.decay</code> (float) The decay rate for EMA. Its range must be in [0, 1.0]. If <code>None</code>. Constant decay example <pre><code>training:\n  ema:\n    name: constant_decay\n    decay: 0.9999\n</code></pre>"},{"location":"components/training/ema/#exponential-decay","title":"Exponential decay","text":"<p>Exponential decay increases the decay value exponentially with the number of updates as following:</p> <pre><code>applied_decay = decay * (1 - math.exp(-counter / beta)\n</code></pre> <p><code>decay</code> and <code>beta</code> from configuration determine the maximum value of decay and the speed of convergence, respectively. The <code>counter</code> starts at 0 and increments by 1 with each update.</p> Field  Description <code>training.name</code> (str) Name must be \"exp_decay\" to use constant decay. <code>training.decay</code> (float) The decay rate for EMA. For exponential decay, this means maximum decay value. Its range must be in [0, 1.0]. <code>training.beta</code> (float) Determines the speed of convergence of decay to maximum value. Exponential decay example <pre><code>training:\n  ema:\n    name: exp_decay\n    decay: 0.9999\n    beta: 100\n</code></pre>"},{"location":"components/training/gradient-clipping/","title":"Gradient Clipping","text":"<p>Gradient clipping is a technique used to mitigate the exploding gradient problem in training deep neural networks, particularly in scenarios involving recurrent neural networks (RNNs) or models with long dependency chains. </p> <p>During the training of neural networks, especially those with deep architectures or recurrent structures, gradients can sometimes grow exponentially large. This phenomenon, known as exploding gradients, can lead to several issues as follows:</p> <ul> <li>Numerical instability</li> <li>Overshooting optimal parameter values</li> <li>Divergence in the training process</li> </ul> <p>Gradient clipping addresses this issue by limiting the magnitude of gradients during backpropagation. This technique ensures that gradient updates remain within a reasonable range, promoting more stable and controlled training.</p>"},{"location":"components/training/gradient-clipping/#norm-clipping","title":"Norm Clipping","text":"<p>For now, netspresso-trainer supports the norm clipping. This method scales down the gradient when its norm exceeds a threshold \\(v\\). $$ \\text{if } ||\\mathbf{g}|| &gt; v \\text{ then } \\mathbf{g} \\leftarrow v \\cdot \\frac{\\mathbf{g}}{||\\mathbf{g}||} $$</p> Field  Description <code>training.max_norm</code> (float) The norm threshold. For gradient clipping, this means the maximum gradient value. To disable the gradient clipping, you can set this value to None (~). Norm gradient clipping example <pre><code>training:\n    max_norm: 0.1\n</code></pre>"},{"location":"components/training/optimizers/","title":"Optimizers","text":"<p>NetsPresso Trainer uses the optimizers implemented in PyTorch as is. By selecting an optimizer that suits your training recipe, you can configure the optimal training. If you are unsure which optimizer to use, we recommend reading the blog post from towardsdatascience.</p>"},{"location":"components/training/optimizers/#supporting-optimizers","title":"Supporting optimizers","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/training/optimizers/#adadelta","title":"Adadelta","text":"<p>This optimizer follows the Adadelta in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adadelta\" to use <code>Adadelta</code> optimizer. <code>lr</code> (float) Coefficient that scales delta before it is applied to the parameters. <code>rho</code> (float) Coefficient used for computing a running average of squared gradients <code>weight_decay</code> (float) weight decay (L2 penalty). Adadelta example <pre><code>training:\n  optimizer:\n    name: adadelta\n    lr: 1.0\n    rho: 0.9\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adagrad","title":"Adagrad","text":"<p>This optimizer follows the Adagrad in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adagrad\" to use <code>Adagrad</code> optimizer. <code>lr</code> (float) Learning rate. <code>lr_decay</code> (float) Learning rate decay. <code>weight_decay</code> (float) weight decay (L2 penalty). Adagrad example <pre><code>training:\n  optimizer:\n    name: adagrad\n    lr: 1e-2\n    lr_decay: 0.\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adam","title":"Adam","text":"<p>This optimizer follows the Adam (<code>adam</code>) in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adam\" to use <code>Adam</code> optimizer. <code>lr</code> (float) Learning rate. <code>betas</code> (float) Coefficients used for computing running averages of gradient and its square. <code>weight_decay</code> (float) weight decay (L2 penalty). Adam example <pre><code>training:\n  optimizer:\n    name: adam\n    lr: 1e-3\n    betas: [0.9, 0.999]\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adamax","title":"Adamax","text":"<p>This optimizer follows the Adamax in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adamax\" to use <code>Adamax</code> optimizer. <code>lr</code> (float) Learning rate. <code>betas</code> (float) Coefficients used for computing running averages of gradient and its square. <code>weight_decay</code> (float) weight decay (L2 penalty). Adamax example <pre><code>training:\n  optimizer:\n    name: adamax\n    lr: 2e-3\n    betas: [0.9, 0.999]\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#adamw","title":"AdamW","text":"<p>This optimizer follows the AdamW in torch library.</p> Field  Description <code>name</code> (str) Name must be \"adamw\" to use <code>AdamW</code> optimizer. <code>lr</code> (float) Learning rate. <code>betas</code> (list[float]) Coefficients used for computing running averages of gradient and its square. <code>weight_decay</code> (float) weight decay (L2 penalty). AdamW example <pre><code>training:\n  optimizer:\n    name: adamw\n    lr: 1e-3\n    betas: [0.9, 0.999]\n    weight_decay: 0.\n</code></pre>"},{"location":"components/training/optimizers/#rmsprop","title":"RMSprop","text":"<p>This optimizer follows the RMSprop in torch library.</p> Field  Description <code>name</code> (str) Name must be \"rmsprop\" to use <code>RMSprop</code> optimizer. <code>lr</code> (float) Learning rate. <code>alpha</code> (float) Smoothing constant. <code>momentum</code> (float) Momentum factor. <code>weight_decay</code> (float) weight decay (L2 penalty). <code>eps</code> (float) Term added to the denominator to improve numerical stability. RMSprop example <pre><code>training:\n  optimizer:\n    name: rmsprop\n    lr: 1e-2\n    alpha: 0.99\n    momentum: 0.\n    weight_decay: 0.\n    eps: 1e-8\n</code></pre>"},{"location":"components/training/optimizers/#sgd","title":"SGD","text":"<p>This optimizer follows the SGD in torch library.</p> Field  Description <code>name</code> (str) Name must be \"sgd\" to use <code>SGD</code> optimizer. <code>lr</code> (float) Learning rate. <code>momentum</code> (float) Momentum factor. <code>weight_decay</code> (float) weight decay (L2 penalty). <code>nesterov</code> (bool) Enables Nesterov momentum. SGD example <pre><code>training:\n  optimizer:\n    name: sgd\n    lr: 1e-2\n    momentum: 0.\n    weight_decay: 0.\n    nesterov: false\n</code></pre>"},{"location":"components/training/overview/","title":"Overview","text":"<p>In training, the training recipe is just as important as the model architecture. Even if you have a good model architecture, the performance on the same data and model combination can vary greatly depending on your training recipe. NetsPresso Trainer not only introduces models optimized for edge devices, but also provides the ability to change training configurations to train these models with various data. The optimal training recipe will vary depending on the data you want to train. Use the options provided by NetsPresso Trainer to find the optimal training recipe for your data.</p> <p>Users can adjust epochs, the desired optimizer and scheduler as a following example.</p> <pre><code>training:\n  epochs: 300\n  ema:\n    name: constant_decay\n    decay: 0.9999\n  optimizer:\n    name: adamw\n    lr: 6e-5\n    betas: [0.9, 0.999]\n    weight_decay: 0.0005\n  scheduler:\n    name: cosine_no_sgdr\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 0.\n</code></pre>"},{"location":"components/training/overview/#field-list","title":"Field list","text":"Field  Description <code>training.epochs</code> (int) The total number of epoch for training the model <code>training.ema</code> (dict, optional) The configuration of EMA. Please refer to the EMA page for more details. If <code>None</code>, EMA is not applied. <code>training.optimizer</code> (dict) The configuration of optimizer. Please refer to the list of supporting optimizer for more details. <code>training.scheduler</code> (dict) The configuration of learning rate scheduler. Please refer to the list of supporting scheduler for more details."},{"location":"components/training/schedulers/","title":"Schedulers","text":"<p>NetsPresso Trainer supports various learning rate schedulers based on PyTorch. In particular, learning rate warm-up is supported for frequently used schedulers, and learning rate restart is supported for some schedulers, such as cosine annealing. NetsPresso Trainer updates the learning rate at the end of epoch, not the end of step, so users will set the scheduler with epoch-level counts.</p>"},{"location":"components/training/schedulers/#supporting-schedulers","title":"Supporting schedulers","text":"<p>The currently supported methods in NetsPresso Trainer are as follows. Since techniques are adapted from pre-existing codes, most of the parameters remain unchanged. We note that most of these parameter descriptions are derived from original implementations.</p> <p>We appreciate all the original code owners and we also do our best to make other values.</p>"},{"location":"components/training/schedulers/#step","title":"Step","text":"<p>This scheduler follows the StepLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"step\" to use <code>StepLR</code> scheduler. <code>iters_per_phase</code> (int) Epoch period of learning rate decay. <code>gamma</code> (float) Multiplicative factor of learning rate decay. <code>end_epoch</code> (int) End epoch of this scheduler. Remained epochs will be trained with fixed learning rate. Step example <pre><code>training:\n  scheduler:\n    name: step\n    iters_per_phase: 1\n    gamma: 0.1\n    end_epoch: 80\n</code></pre>"},{"location":"components/training/schedulers/#polynomial-with-warmup","title":"Polynomial with warmup","text":"<p>This scheduler follows the PolynomialLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"poly\" to use <code>PolynomialLRWithWarmUp</code> scheduler. <code>warmup_epochs</code> (int) The number of steps that the scheduler finishes to warmup the learning rate. <code>warmup_bias_lr</code> (float) Starting learning rate for warmup period. <code>min_lr</code> (float) Minimum learning rate. <code>power</code> (float) The power of the polynomial. <code>end_epoch</code> (int) End epoch of this scheduler. At the <code>end_epoch</code>, learning rate will be <code>min_lr</code>, and remained epochs trained with fixed learning rate. Polynomial with warmup example <pre><code>training:\n  scheduler:\n    name: poly\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 1e-6\n    power: 1.0\n    end_epoch: 80\n</code></pre>"},{"location":"components/training/schedulers/#cosine-annealing-with-warmup","title":"Cosine annealing with warmup","text":"<p>This scheduler follows the CosineAnnealingLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"cosine_no_sgdr\" to use <code>CosineAnnealingLRWithCustomWarmUp</code> scheduler. <code>warmup_epochs</code> (int) The number of steps that the scheduler finishes to warmup the learning rate. <code>warmup_bias_lr</code> (float) Starting learning rate for warmup period. <code>min_lr</code> (float) Minimum learning rate. <code>end_epoch</code> (int) End epoch of this scheduler. At the <code>end_epoch</code>, learning rate will be <code>min_lr</code>, and remained epochs trained with fixed learning rate. Cosine annealing with warmup example <pre><code>training:\n  scheduler:\n    name: cosine_no_sgdr\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 1e-6\n    end_epoch: 80\n</code></pre>"},{"location":"components/training/schedulers/#cosine-annealing-warm-restarts-with-warmup","title":"Cosine annealing warm restarts with warmup","text":"<p>This scheduler follows the CosineAnnealingWarmRestarts in torch library.</p> Field  Description <code>name</code> (str) Name must be \"cosine\" to use <code>CosineAnnealingWarmRestartsWithCustomWarmUp</code> scheduler. <code>warmup_epochs</code> (int) The number of steps that the scheduler finishes to warmup the learning rate. <code>warmup_bias_lr</code> (float) Starting learning rate for warmup period. <code>min_lr</code> (float) Minimum learning rate. <code>iters_per_phase</code> (float) Epoch period for the learning rate restart. Cosine annealing warm restart with warmup example <pre><code>training:\n  scheduler:\n    name: cosine\n    warmup_epochs: 5\n    warmup_bias_lr: 1e-5\n    min_lr: 1e-6\n    iters_per_phase: 10\n</code></pre>"},{"location":"components/training/schedulers/#multi-step","title":"Multi step","text":"<p>This scheduler follows the MultiStepLR in torch library.</p> Field  Description <code>name</code> (str) Name must be \"multi_step\" to use <code>MultiStepLR</code> scheduler. <code>milestones</code> (list) List of epoch indices. Must be increasing. <code>gamma</code> (float) Multiplicative factor of learning rate decay. Step example <pre><code>training:\n  scheduler:\n    name: multi_step\n    milestones: [30, 80]\n    gamma: 0.1\n</code></pre>"},{"location":"components/training/schedulers/#gradio-demo-for-simulating-the-learning-rate-scheduler","title":"Gradio demo for simulating the learning rate scheduler","text":"<p>In many training feature repositories, it is recommended to perform the entire training pipeline and check the log to see how the learning rate scheduler works. NetsPresso Trainer supports learning rate schedule simulation to allow users to easily understand the learning rate scheduler for their configured training recipe. By copying and pasting the training configuration into the simulator, users can see how the learning rate changes every epoch.</p> <p> This simulation is not supported for some schedulers which adjust the learning rate dynamically with training results.</p>"},{"location":"components/training/schedulers/#running-on-your-environment","title":"Running on your environment","text":"<p>Please run the gradio demo with following command:</p> <pre><code>bash scripts/run_simulator_lr_scheduler.sh\n</code></pre>"},{"location":"getting_started/simple_use/","title":"Simple use","text":""},{"location":"getting_started/simple_use/#training","title":"Training","text":"<p>Write your training script in <code>train.py</code> like:</p> <pre><code>from netspresso_trainer import train_cli\n\nif __name__ == '__main__':\n    logging_dir = train_cli()\n    print(f\"Training results are saved at: {logging_dir}\")\n</code></pre> <p>Then, train your model with your own configuraiton:</p> <pre><code>python train.py\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --training config/training.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre> <p>Or you can start NetsPresso Trainer by just executing console script which has same feature.</p> <pre><code>netspresso-train\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --training config/training.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre> <p>Please refer to <code>scripts/example_train.sh</code>.</p> <p>NetsPresso Trainer is compatible with NetsPresso service. We provide NetsPresso Trainer tutorial that contains whole procedure from model train to model compression and benchmark. Please refer to our colab tutorial.</p>"},{"location":"getting_started/simple_use/#evaluation","title":"Evaluation","text":"<p>Write your evaluation script in <code>evaluation.py</code> like:</p> <pre><code>from netspresso_trainer import evaluation_cli\n\nif __name__ == '__main__':\n    logging_dir = evaluation_cli()\n\n    print(f\"Evaluation results are saved at: {logging_dir}\")\n</code></pre> <p>Then, evaluate your model with your own configuraiton:</p> <pre><code>python evaluation.py\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre>"},{"location":"getting_started/simple_use/#inference","title":"Inference","text":"<p>Write your inference script in <code>inference.py</code> like:</p> <pre><code>from netspresso_trainer import inference_cli\n\nif __name__ == '__main__':\n    logging_dir = inference_cli()\n\n    print(f\"Inference results are saved at: {logging_dir}\")\n</code></pre> <p>Then, inference your dataset:</p> <pre><code>python inference.py\\\n  --data config/data/huggingface/beans.yaml\\\n  --augmentation config/augmentation/classification.yaml\\\n  --model config/model/resnet/resnet50-classification.yaml\\\n  --logging config/logging.yaml\\\n  --environment config/environment.yaml\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/","title":"Data preparation (Hugging Face)","text":"<p>The Hugging Face datasets offers a vast array of datasets to support various tasks, making them readily accessible through a user-friendly API. Provided datasets by Hugging Face datasets are typically structured into <code>training</code>, <code>validation</code>, and <code>testing</code> sets. This structure allows NetsPresso Trainer to utilize various datasets with yaml configuration.</p> <p>To explore official Hugging Face datasets catalogue, please refer to the hugging Face datasets page.</p>"},{"location":"getting_started/dataset_preparation/huggingface/#hugging-face-datasets-install","title":"Hugging Face datasets install","text":"<p>First, you must install the hugging Face datasets library.</p> <pre><code>pip install -r requirements-optional.txt\n\nor\n\npip install datasets\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#find-dataset-repository","title":"Find dataset repository","text":"<p>We use CIFAR100 dataset as an example. Thus, <code>format</code> field in data configuration is filled as huggingface, and <code>metadata.repo</code> is filled as <code>cifar100</code>.</p> <pre><code>data:\n  name: cifar100\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: cifar100\n    subset: ~\n    features:\n      image: ~\n      label: ~\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#agreement-the-conditions-to-access-the-datasets","title":"Agreement the conditions to access the datasets","text":"<p>Some datasets are publicly available, but may require a agreement to be used. For instance, to use ImageNet1K in Hugging Face datasets, you have to log in to Hugging Face homepage and accpet the conditions.</p> <p>Make sure that you agreed to the conditions on Hugging Face website, and log in to <code>huggingface-cli</code> before you start.</p> <pre><code>huggingface-cli login\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#set-subset","title":"Set subset","text":"<p>Some datasets have multiple subsets in their dataset hierarchy. They have to be specified in <code>subset</code> field.</p> <p>If there is no subset in the dataset, you can leave <code>subset</code> field as null.</p> <pre><code>data:\n  name: cifar100\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: cifar100\n    subset: ~ # You have to fill this field if there is subset in the dataset you trying to use\n    features:\n      image: ~\n      label: ~\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#set-features","title":"Set features","text":"<p>You should check features of dataset in Hugging Face homepage. If you see CIFAR100, there are three features in the dataset which are <code>img</code>, <code>fine_label</code>, <code>coarse_label</code>. In this dataset, the image data is denoted by <code>img</code> and the labels for the 100 classes are represented by <code>fine_label</code>. Given this structure, the data configuration should be filled out as below.</p> <pre><code>data:\n  name: cifar100\n  task: classification\n  format: huggingface\n  metadata:\n    custom_cache_dir: ./data/huggingface \n    repo: cifar100\n    subset: ~ # You should fill this field if there is subset in the dataset you trying to use\n    features:\n      image: img\n      label: fine_label\n</code></pre>"},{"location":"getting_started/dataset_preparation/huggingface/#run-netspresso-trainer","title":"Run NetsPresso Trainer","text":"<p>Now you can run NetsPresso Trainer with Hugging Face dataset!</p> <pre><code>python train.py --data your_huggingface_dataset_yaml_path.yaml ...\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/","title":"Data preparation (Local)","text":""},{"location":"getting_started/dataset_preparation/local/#local-custom-datasets","title":"Local custom datasets","text":"<p>If your dataset is ready in local storage, you can use them by following the instructions.</p>"},{"location":"getting_started/dataset_preparation/local/#organize-dataset","title":"Organize dataset","text":"<p>Create separate directories for images/labels and train/valid/test.</p> <pre><code>/my_dataset\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 valid\n\u2502   \u2514\u2500\u2500 test\n\u2514\u2500\u2500 labels\n</code></pre> <p>Place your images on proper path.</p> <pre><code>/my_dataset\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 valid\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u251c\u2500\u2500 img1.jpg\n\u2502       \u251c\u2500\u2500 img2.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n</code></pre> <p>Set labels on proper path.</p> <ul> <li>For image classification, you may need csv format label files.</li> <li>For semantic segmentation and object detection, organize your label files (could be masks or box annotations) in corresponding folders.</li> </ul> <pre><code>/my_dataset\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 valid\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 test\n\u2502       \u251c\u2500\u2500 img1.jpg\n\u2502       \u251c\u2500\u2500 img2.jpg\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 labels\n    \u2514\u2500\u2500 train directory or file ...\n    \u251c\u2500\u2500 valid directory or file ...\n    \u2514\u2500\u2500 test directory or file ...\n</code></pre> <p>If you just run training, test split may not needed.</p> <p>If you just run evaluation or inference, train and valid split may not needed.</p>"},{"location":"getting_started/dataset_preparation/local/#set-configuration-file","title":"Set configuration file","text":"<p>Define the paths to your datasets in the configuration file to tell NetsPresso Trainer where to find the data. Finally, you can complete data configuration by adding some metadata like <code>id_mapping</code>. Here is example for classification:</p> <pre><code>data:\n  name: my_custom_dataset\n  task: classification # This could be other task\n  format: local\n  path:\n    root: ./my_dataset\n    train:\n      image: train/images\n      label: train/labels.csv\n    valid:\n      image: valid/images\n      label: valid/labels.csv\n    test:\n      image: test/images\n      label: test/labels.csv\n  id_mapping: [cat, dog, elephant]\n</code></pre> <p>For detailed definition of data configuration, please refer to components/data</p>"},{"location":"getting_started/dataset_preparation/local/#open-datasets","title":"Open datasets","text":"<p>If you are interested in using open datasets, follow the instructions below to seamlessly integrate them into the local custom datasets format.</p>"},{"location":"getting_started/dataset_preparation/local/#image-classification","title":"Image classification","text":""},{"location":"getting_started/dataset_preparation/local/#cifar100","title":"CIFAR100","text":"<p>Run <code>cifar100.py</code> python file with your dataset directory as an argument.</p> <p>CIFAR100 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/cifar100.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#imagenet1k","title":"ImageNet1K","text":"<p>ImageNet1K dataset cannot be automatically downloaded. You should download dataset from ImageNet website, and place downloaded files into <code>./data/download</code>.</p> <p>And, run <code>imagenet1k.py</code> python file with your dataset directorty and downloaded files path as arguments. After executing scripts, you can use pre-defined configuration.</p> <p>(<code>imagenet1k.py</code> needs scipy library which is in requirements-optional.txt)</p> <pre><code>python ./tools/open_dataset_tool/imagenet1k.py --dir ./data --train-images ./data/download/ILSVRC2012_img_train.tar --valid-images ./data/download/ILSVRC2012_img_val.tar --devkit ./data/download/ILSVRC2012_devkit_t12.tar.gz\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#semantic-segmentation","title":"Semantic segmentation","text":""},{"location":"getting_started/dataset_preparation/local/#ade20k","title":"ADE20K","text":"<p>Run <code>ade20k.py</code> python file with your dataset directory as an augument.</p> <p>ADE20K dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/ade20k.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#cityscapes","title":"Cityscapes","text":"<p>Cityscapes dataset cannot be automatically downloaded. You should download dataset from Cityscapes website, and place downloaded files into <code>./data/download</code>.</p> <p>And, run <code>cityscapes.py</code> python file with your dataset directorty and downloaded files path as arguments. After executing scripts, you can use pre-defined configuration.</p> <pre><code>python --dir ./data --images .data/download/leftImg8bit_trainvaltest.zip --labels .data/download/gtFine_trainvaltest.zip\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#pascalvoc-2012","title":"PascalVOC 2012","text":"<p>Run <code>voc2012_seg.py</code> python file with your dataset directory as an argument.</p> <p>PascalVOC 2012 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/voc2012_seg.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#object-detection","title":"Object detection","text":""},{"location":"getting_started/dataset_preparation/local/#coco-2017","title":"COCO 2017","text":"<p>Run <code>coco2017.py</code> python file with your dataset directory as an argument.</p> <p>COCO 2017 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/coco2017.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#objects365","title":"Objects365","text":"<p>Run <code>objects365.py</code> python file with your dataset directory as an argument.</p> <p>Objects365 dataset will be automatically downloaded to <code>./data/download/objects365</code>. After executing scripts, you can use pre-defined configuration. As the dataset is quite large, It is recommened to use multiprocess when you download it (e.g., <code>--num_process 4</code>).</p> <pre><code>python ./tools/open_dataset_tool/objects365.py --dir ./data --num_process 4\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#pascalvoc-2012_1","title":"PascalVOC 2012","text":"<p>Run <code>voc2012_det.py</code> python file with your dataset directory as an argument.</p> <p>PascalVOC 2012 dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/voc2012_det.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#pose-estimation","title":"Pose estimation","text":""},{"location":"getting_started/dataset_preparation/local/#wflw","title":"WFLW","text":"<p>Run <code>wflw.py</code> python file with your dataset directory as an argument.</p> <p>WFLW dataset will be automatically downloaded to <code>./data/download</code>. After executing scripts, you can use  pre-defined configuration.</p> <pre><code>python ./tools/open_dataset_tool/wflw.py --dir ./data\n</code></pre>"},{"location":"getting_started/dataset_preparation/local/#run-netspresso-trainer","title":"Run NetsPresso Trainer","text":"<p>Now you can run NetsPresso Trainer with your local dataset!</p> <pre><code>python train.py --data your_huggingface_dataset_yaml_path.yaml ...\n</code></pre>"},{"location":"getting_started/installation/docker_installation/","title":"Setup with Docker","text":""},{"location":"getting_started/installation/docker_installation/#installation-with-docker","title":"Installation with docker","text":""},{"location":"getting_started/installation/docker_installation/#docker-with-docker-compose","title":"Docker with docker-compose","text":"<p>For the latest information, please check <code>docker-compose.yml</code></p> <pre><code># run command\nexport TAG=v$(cat src/netspresso_trainer/VERSION) &amp;&amp; \\\ndocker compose run --service-ports --name netspresso-trainer-dev netspresso-trainer bash\n</code></pre>"},{"location":"getting_started/installation/docker_installation/#docker-image-build","title":"Docker image build","text":"<p>If you run with <code>docker run</code> command, follow the image build and run command in the below:</p> <pre><code># build an image\ndocker build -t netspresso-trainer:v$(cat src/netspresso_trainer/VERSION) .\n</code></pre> <pre><code># docker run command\ndocker run -it --ipc=host\\\n  --gpus='\"device=0,1,2,3\"'\\\n  -v /PATH/TO/DATA:/DATA/PATH/IN/CONTAINER\\\n  -v /PATH/TO/CHECKPOINT:/CHECKPOINT/PATH/IN/CONTAINER\\\n  -p 50001:50001\\\n  -p 50002:50002\\\n  -p 50003:50003\\\n  --name netspresso-trainer-dev netspresso-trainer:v$(cat src/netspresso_trainer/VERSION)\n</code></pre>"},{"location":"getting_started/installation/installation/","title":"Installation (Stable)","text":""},{"location":"getting_started/installation/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python <code>&gt;=3.10</code></li> <li>PyTorch <code>&gt;=2.0.1</code></li> </ul>"},{"location":"getting_started/installation/installation/#install-with-pypi","title":"Install with pypi","text":"<pre><code>pip install netspresso_trainer\n</code></pre>"},{"location":"getting_started/installation/installation/#install-with-github","title":"Install with GitHub","text":"<p>To use pip install,</p> <pre><code>pip install git+https://github.com/Nota-NetsPresso/netspresso-trainer.git@master\n</code></pre> <p>To install with editable mode,</p> <pre><code>git clone -b master https://github.com/Nota-NetsPresso/netspresso-trainer.git\npip install -e netspresso-trainer\n</code></pre>"},{"location":"models/overview/","title":"Overview","text":"<p>This section describes the architecture configuration design of models. For details on NetsPresso Trainer's model configuration excluding the architecture, please refer to the model components page.</p> <p>NetsPresso Trainer prioritize model compression and device deployment, thus models fulfill the following criteria:</p> <ul> <li>Compatible with torch.fx converting.</li> <li>Can be compressed by pruning method provided in NetsPresso.</li> <li>Can be easily deployed at many edge devices.</li> </ul> <p>To provide a wide range of models that meet these conditions in diverse forms, we define and use four fields for model definition: full, backbone, neck, and head. This approach allows users to utilize backbones, necks, and heads in desired configurations. For models that cannot be segmented into these three modules, we provide them in a full models.</p> <pre><code>model:\n  architecture:\n    full: ~ # For full model which can't be separated to backbone, neck and head.\n    backbone: ~ # Model backbone configuration.\n    neck: ~ # Model neck configuration.\n    head: ~ # Model head configuration.\n</code></pre>"},{"location":"models/overview/#field-list","title":"Field list","text":"Field  Description <code>full</code> (dict) If the model does not distinctly separated to backbone, neck, and head, the model's details are defined under this field. If this field is not <code>None</code>, the <code>backbone</code>, <code>neck</code>, and <code>head</code> fields are ignored. <code>backbone</code> (dict) This field defines the model's backbone, applicable only when the <code>full</code> field is <code>None</code>. <code>neck</code> (dict) This field defines the model's neck, applicable only when the <code>full</code> field is <code>None</code>. This can be <code>None</code> anytime because the necessity of the neck module may vary depending on the task. <code>head</code> (dict) This field defines the model's head, applicable only when the <code>full</code> field is <code>None</code>."},{"location":"models/backbones/cspdarknet/","title":"CSPDarkNet","text":"<p>CSPDarkNet backbone based on YOLOX: Exceeding YOLO Series in 2021.</p> <p>CSPDarkNet is a modified model from Darknet53 by adopting the strategy of CSPNet. Therefore, the structure of the model is fixed, and neither the number of stages nor type of blocks can be changed. The size of the model is determined by two values, which define the feature dimensions within the model and the repetition of CSPLayers.</p>"},{"location":"models/backbones/cspdarknet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"cspdarknet\" to use <code>CSPDarkNet</code> backbone. <code>params.depthwise</code> (bool) Whether to enable depthwise convolution for the <code>CSPDarkNet</code> backbone. <code>params.dep_mul</code> (float) Multiplying factor determining the repetition count of <code>CSPLayer</code> in the backbone. <code>params.wid_mul</code> (float) Multiplying factor adjusting the input/output dimensions of convolutional layers throughout the backbone. <code>params.act_type</code> (str) Type of activation function for the model. Supporting activation functions are described in [here]."},{"location":"models/backbones/cspdarknet/#model-configuration-examples","title":"Model configuration examples","text":"CSPDarkNet-nano <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        depthwise: True\n        dep_mul: &amp;dep_mul 0.33\n        wid_mul: 0.25\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-tiny <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        depthwise: False\n        dep_mul: &amp;dep_mul 0.33\n        wid_mul: 0.375\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-s <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        depthwise: False\n        dep_mul: &amp;dep_mul 0.33\n        wid_mul: 0.5\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-m <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        depthwise: False\n        dep_mul: &amp;dep_mul 0.67\n        wid_mul: 0.75\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-l <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        depthwise: False\n        dep_mul: &amp;dep_mul 1.0\n        wid_mul: 1.0\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre> CSPDarkNet-x <pre><code>model:\n  architecture:\n    backbone:\n      name: cspdarknet\n      params:\n        depthwise: False\n        dep_mul: &amp;dep_mul 1.33\n        wid_mul: 1.25\n        act_type: &amp;act_type \"silu\"\n      stage_params: ~\n</code></pre>"},{"location":"models/backbones/cspdarknet/#related-links","title":"Related links","text":"<ul> <li><code>Megvii-BaseDetection/YOLOX</code></li> </ul>"},{"location":"models/backbones/efficientformer/","title":"EfficientFormer","text":"<p>EfficientFormer backbone based on EfficientFormer: Vision Transformers at MobileNet Speed.</p> <p>EfficientFormer is designed following the design principle of MetaFormer, constructing its backbone by stacking MetaBlocks. 4D MetaBlocks are employed throughout the model, and 3D MetaBlocks are used at the end of the backbone to enhance the model's expression power. We provide configuration options to adjust the design settings including repetition values for 3D and 4D MetaBlocks.</p>"},{"location":"models/backbones/efficientformer/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"efficientformer\" to use EfficientFormer backbone. <code>params.num_attention_heads</code> (int) The number of heads in the multi-head attention of 3D MetaBlock. <code>params.attention_channels</code> (int) Dimension for attention of 3D MetaBlock. <code>params.attention_dropout_prob</code> (float) Dropout probability for attention of 3D MetaBlock. <code>params.attention_value_expansion_ratio</code> (int) Value dimension expansion ratio of 3D MetaBlock. <code>params.ffn_intermediate_ratio</code> (int) Dimension expansion ratio of MLP layer in 3D and 4D MetaBlock. <code>params.ffn_dropout_prob</code> (float) Dropout probability of MLP layer in 3D and 4D MetaBlock. <code>params.ffn_act_type</code> (str) Activation function of MLP layer in 3D and 4D MetaBlocks <code>params.vit_num</code> (int) The number of last 3D MetaBlock. <code>stage_params[n].num_blocks</code> (int) The number of 4D MetaBlock in the stage. <code>stage_params[n].channels</code> (int) Dimensions for 4D MetaBlock in the stage."},{"location":"models/backbones/efficientformer/#model-configuration-examples","title":"Model configuration examples","text":"EfficientFormer-L1 <pre><code>model:\n  architecture:\n    backbone:\n      name: efficientformer\n      params:\n        num_attention_heads: 8\n        attention_channels: 256  # attention_hidden_size_splitted * num_attention_heads\n        attention_dropout_prob: 0.\n        attention_value_expansion_ratio: 4\n        ffn_intermediate_ratio: 4\n        ffn_dropout_prob: 0.\n        ffn_act_type: 'gelu'\n        vit_num: 1\n      stage_params:\n        - \n          num_blocks: 3\n          channels: 48\n        - \n          num_blocks: 2\n          channels: 96\n        - \n          num_blocks: 6\n          channels: 224\n        - \n          num_blocks: 4\n          channels: 448\n</code></pre> EfficientFormer-L3 <pre><code>model:\n  architecture:\n    backbone:\n      name: efficientformer\n      params:\n        num_attention_heads: 8\n        attention_channels: 256  # attention_hidden_size_splitted * num_attention_heads\n        attention_dropout_prob: 0.\n        attention_value_expansion_ratio: 4\n        ffn_intermediate_ratio: 4\n        ffn_dropout_prob: 0.\n        ffn_act_type: 'gelu'\n        vit_num: 4\n      stage_params:\n        - \n          num_blocks: 4\n          channels: 64\n        - \n          num_blocks: 4\n          channels: 128\n        - \n          num_blocks: 12\n          channels: 320\n        - \n          num_blocks: 6\n          channels: 512\n</code></pre>"},{"location":"models/backbones/efficientformer/#related-links","title":"Related links","text":"<ul> <li><code>snap-research/EfficientFormer</code></li> </ul>"},{"location":"models/backbones/gelan/","title":"GELAN (Generalized Efficient Layer Aggregation Network)","text":"<p>GELAN backbone based on YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information.</p>"},{"location":"models/backbones/gelan/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"gelan\" to use <code>GELAN</code> backbone. <code>stage_params[n] case1: Conv2D</code> (list) Build <code>Conv2D</code> layer under following format: <code>['conv', out_channels, kernel_size, stride]</code>. <code>stage_params[n] case2: ELAN</code> (list) Build <code>ELAN</code> block under following format: <code>['elan', out_channels, part_channels, use_identity]</code>. <code>stage_params[n] case3: RepNCSPELAN</code> (list) Build <code>RepNCSPELAN</code> block under following format: <code>['repncspelan', out_channels, part_channels, use_identity, depth]</code>. <code>stage_params[n] case4: AConv</code> (list) Build <code>AConv</code> block under following format: <code>['aconv', out_channels]</code>. <code>stage_params[n] case4: ADown</code> (list) Build <code>ADown</code> block under following format: <code>['adown', out_channels]</code>."},{"location":"models/backbones/gelan/#model-configuration-examples","title":"Model configuration examples","text":"GELAN-tiny <pre><code>model:\n  architecture:\n    backbone:\n      name: gelan\n      params:\n        stem_out_channels: 16\n        stem_kernel_size: 3\n        stem_stride: 2\n        return_stage_idx: ~\n        act_type: &amp;act_type silu\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # ELAN: ['elan', out_channels, part_channels, use_identity]\n        # RepNCSPELAN: ['repncspelan', out_channels, part_channels, use_identity, depth]\n        # AConv: ['aconv', out_channels]\n        # ADown: ['adown', out_channels]\n        -\n          - ['conv', 32, 3, 2]\n          - ['elan', 32, 32, false]\n        -\n          - ['aconv', 64]\n          - ['repncspelan', 64, 64, false, 3]\n        -\n          - ['aconv', 96]\n          - ['repncspelan', 96, 96, false, 3]\n        - \n          - ['aconv', 128]\n          - ['repncspelan', 128, 128, false, 3]\n</code></pre> GELAN-s <pre><code>model:\n  architecture:\n    backbone:\n      name: gelan\n      params:\n        stem_out_channels: 32\n        stem_kernel_size: 3\n        stem_stride: 2\n        return_stage_idx: ~\n        act_type: &amp;act_type silu\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # ELAN: ['elan', out_channels, part_channels, use_identity]\n        # RepNCSPELAN: ['repncspelan', out_channels, part_channels, use_identity, depth]\n        # AConv: ['aconv', out_channels]\n        # ADown: ['adown', out_channels]\n        -\n          - ['conv', 64, 3, 2]\n          - ['elan', 64, 64, false]\n        -\n          - ['aconv', 128]\n          - ['repncspelan', 128, 128, false, 3]\n        -\n          - ['aconv', 192]\n          - ['repncspelan', 192, 192, false, 3]\n        - \n          - ['aconv', 256]\n          - ['repncspelan', 256, 256, false, 3]\n</code></pre> GELAN-m <pre><code>model:\n  architecture:\n    backbone:\n      name: gelan\n      params:\n        stem_out_channels: 32\n        stem_kernel_size: 3\n        stem_stride: 2\n        return_stage_idx: ~\n        act_type: &amp;act_type silu\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # ELAN: ['elan', out_channels, part_channels, use_identity]\n        # RepNCSPELAN: ['repncspelan', out_channels, part_channels, use_identity, depth]\n        # AConv: ['aconv', out_channels]\n        # ADown: ['adown', out_channels]\n        -\n          - ['conv', 64, 3, 2]\n          - ['repncspelan', 128, 128, false, 1]\n        -\n          - ['aconv', 240]\n          - ['repncspelan', 240, 240, false, 1]\n        -\n          - ['aconv', 360]\n          - ['repncspelan', 360, 360, false, 1]\n        - \n          - ['aconv', 480]\n          - ['repncspelan', 480, 480, false, 1]\n</code></pre> GELAN-c <pre><code>model:\n  architecture:\n    backbone:\n      name: gelan\n      params:\n        stem_out_channels: 64\n        stem_kernel_size: 3\n        stem_stride: 2\n        return_stage_idx: ~\n        act_type: &amp;act_type silu\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # ELAN: ['elan', out_channels, part_channels, use_identity]\n        # RepNCSPELAN: ['repncspelan', out_channels, part_channels, use_identity, depth]\n        # AConv: ['aconv', out_channels]\n        # ADown: ['adown', out_channels]\n        -\n          - ['conv', 128, 3, 2]\n          - ['repncspelan', 256, 128, false, 1]\n        -\n          - ['adown', 256]\n          - ['repncspelan', 512, 256, false, 1]\n        -\n          - ['adown', 512]\n          - ['repncspelan', 512, 512, false, 1]\n        - \n          - ['adown', 512]\n          - ['repncspelan', 512, 512, false, 1]\n</code></pre>"},{"location":"models/backbones/mixnet/","title":"MixNet","text":"<p>MixNet backbone based on MixConv: Mixed Depthwise Convolutional Kernels.</p> <p>Similar with MobileNetV3, we provide a configuration that allows users to define each MixDepthBlock in MixNet individually. You can specify in a list format the number and form of each MixDepthBlock for every stage. Using this, we pre-construct and provide MixNet family models.</p>"},{"location":"models/backbones/mixnet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mixnet\" to use <code>MixNet</code> backbone. <code>params.stem_channels</code> (int) Output dimension of the first convolution layer. <code>params.wid_mul</code> (float) Ratio for adjusting the input/output dimensions for the entire model. <code>params.dep_mul</code> (float) Ratio for adjusting the <code>num_block</code> value for the entire model. <code>params.dropout_rate</code> (float) Dropout ratio applied to all <code>MixDepthBlock</code>. <code>stage_params[n].expansion_ratio</code> (list[int]) Determines the output dimension of the expansion phase in each <code>MixDepthBlock</code>. Expands the input dimensions by multiplying the <code>expansion_ratio</code>. <code>stage_params[n].out_channels</code> (list[int]) Output dimensions of each <code>MixDepthBlock</code>. <code>stage_params[n].num_blocks</code> (list[int]) Repetition count for each <code>MixDepthBlock</code>. <code>stage_params[n].kernel_sizes</code> (list[list[int]]) Various kernel sizes used within each <code>MixDepthBlock</code>. <code>stage_params[n].num_exp_groups</code> (list[int]) The number of convolution groups in the expansion phase of <code>MixDepthBlock</code>. <code>stage_params[n].num_poi_groups</code> (list[int]) The number of convolution groups in the final point-wise convolution of <code>MixDepthBlock</code>. <code>stage_params[n].stride</code> (list[int]) Stride values for each <code>MixDepthBlock</code>. <code>stage_params[n].act_type</code> (list[str]) Activation function for each <code>MixDepthBlock</code>. Supporting activation functions are described in [here] <code>stage_params[n].se_reduction_ratio</code> (list[int]) Reduction factor for calculating the output dimension of the squeeze-and-excitation block. If <code>None</code>, the squeeze-and-excitation block is not applied."},{"location":"models/backbones/mixnet/#model-configuration-examples","title":"Model configuration examples","text":"MixNet-s <pre><code>model:\n  architecture:\n    backbone:\n      name: mixnet\n      params:\n        stem_channels: 16\n        wid_mul: 1.0\n        dep_mul: 1.0\n        dropout_rate: 0.\n      stage_params: \n        -\n          expansion_ratio: [1, 6, 3]\n          out_channels: [16, 24, 24]\n          num_blocks: [1, 1, 1]\n          kernel_sizes: [[3], [3], [3]]\n          num_exp_groups: [1, 2, 2]\n          num_poi_groups: [1, 2, 2]\n          stride: [1, 2, 1]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          se_reduction_ratio: [~, ~, ~]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [40, 40]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7], [3, 5]]\n          num_exp_groups: [1, 2]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n        -\n          expansion_ratio: [6, 6, 6, 3]\n          out_channels: [80, 80, 120, 120]\n          num_blocks: [1, 2, 1, 2]\n          kernel_sizes: [[3, 5, 7], [3, 5], [3, 5, 7], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1, 2, 2]\n          num_poi_groups: [2, 2, 2, 2]\n          stride: [2, 1, 1, 1]\n          act_type: [\"swish\", \"swish\", \"swish\", \"swish\"]\n          se_reduction_ratio: [4, 4, 2, 2]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [200, 200]\n          num_blocks: [1, 2]\n          kernel_sizes: [[3, 5, 7, 9, 11], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n</code></pre> MixNet-m <pre><code>model:\n  architecture:\n    backbone:\n      name: mixnet\n      params:\n        stem_channels: 24\n        wid_mul: 1.0\n        dep_mul: 1.0\n        dropout_rate: 0.\n      stage_params: \n        -\n          expansion_ratio: [1, 6, 3]\n          out_channels: [24, 32, 32]\n          num_blocks: [1, 1, 1]\n          kernel_sizes: [[3], [3, 5, 7], [3]]\n          num_exp_groups: [1, 2, 2]\n          num_poi_groups: [1, 2, 2]\n          stride: [1, 2, 1]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          se_reduction_ratio: [~, ~, ~]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [40, 40]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5]]\n          num_exp_groups: [1, 2]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n        -\n          expansion_ratio: [6, 6, 6, 3]\n          out_channels: [80, 80, 120, 120]\n          num_blocks: [1, 3, 1, 3]\n          kernel_sizes: [[3, 5, 7], [3, 5, 7, 9], [3], [3, 5, 7, 9]]\n          num_exp_groups: [1, 2, 1, 2]\n          num_poi_groups: [1, 2, 1, 2]\n          stride: [2, 1, 1, 1]\n          act_type: [\"swish\", \"swish\", \"swish\", \"swish\"]\n          se_reduction_ratio: [4, 4, 2, 2]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [200, 200]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n</code></pre> MixNet-l <pre><code>model:\n  architecture:\n    backbone:\n      name: mixnet\n      params:\n        stem_channels: 24\n        wid_mul: 1.3\n        dep_mul: 1.0\n        dropout_rate: 0.\n      stage_params: \n        -\n          expansion_ratio: [1, 6, 3]\n          out_channels: [24, 32, 32]\n          num_blocks: [1, 1, 1]\n          kernel_sizes: [[3], [3, 5, 7], [3]]\n          num_exp_groups: [1, 2, 2]\n          num_poi_groups: [1, 2, 2]\n          stride: [1, 2, 1]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          se_reduction_ratio: [~, ~, ~]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [40, 40]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5]]\n          num_exp_groups: [1, 2]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n        -\n          expansion_ratio: [6, 6, 6, 3]\n          out_channels: [80, 80, 120, 120]\n          num_blocks: [1, 3, 1, 3]\n          kernel_sizes: [[3, 5, 7], [3, 5, 7, 9], [3], [3, 5, 7, 9]]\n          num_exp_groups: [1, 2, 1, 2]\n          num_poi_groups: [1, 2, 1, 2]\n          stride: [2, 1, 1, 1]\n          act_type: [\"swish\", \"swish\", \"swish\", \"swish\"]\n          se_reduction_ratio: [4, 4, 2, 2]\n        -\n          expansion_ratio: [6, 6]\n          out_channels: [200, 200]\n          num_blocks: [1, 3]\n          kernel_sizes: [[3, 5, 7, 9], [3, 5, 7, 9]]\n          num_exp_groups: [1, 1]\n          num_poi_groups: [1, 2]\n          stride: [2, 1]\n          act_type: [\"swish\", \"swish\"]\n          se_reduction_ratio: [2, 2]\n</code></pre>"},{"location":"models/backbones/mixnet/#related-links","title":"Related links","text":""},{"location":"models/backbones/mixtransformer/","title":"MixTransformer","text":"<p>MixTransformer backbone based on SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.</p> <p>We provide the MixTransformer encoder (MiT), the backbone of SegFormer, as a freely usable backbone module. Users have the flexibility to configure the transformer encoder for each stage, enabling MiT-b0 to MiT-b5.</p>"},{"location":"models/backbones/mixtransformer/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mixtransformer\" to use <code>MixTransformer</code> backbone. <code>params.ffn_intermediate_expansion_ratio</code> (int) Expansion factor to compute intermediate dimension in feed-forward network. <code>params.ffn_act_type</code> (str) Activation function for feed-forward network in the transformer block. Supporting activation functions are described in [here]. <code>params.ffn_dropout_prob</code> (float) Dropout probability for feed-forward network in the transformer block. <code>params.attention_dropout_prob</code> (float) Dropout probability for attention in the transformer block. <code>stage_params[n].num_blocks</code> (int) The number of transformer blocks in the encoder. <code>stage_params[n].sequence_reduction_ratio</code> (int) Sequence reduction ratio for multi-head attention. <code>stage_params[n].encoder_chananels</code> (int) Dimension for the transformer block. <code>stage_params[n].embedding_patch_sizes</code> (int) Kernel size for convolution layer in overlapping patch embedding. <code>stage_params[n].embedding_strides</code> (int) stride value for convolution layer in overlapping patch embedding. <code>stage_params[n].num_attention_heads</code> (int) The number of heads in the multi-head attention."},{"location":"models/backbones/mixtransformer/#model-configuration-examples","title":"Model configuration examples","text":"MiT-b0 <pre><code>model:\n  architecture:\n    backbone:\n      name: mixtransformer\n      params:\n        ffn_intermediate_expansion_ratio: 4\n        ffn_act_type: \"gelu\"\n        ffn_dropout_prob: 0.0\n        attention_dropout_prob: 0.0\n      stage_params:\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 8\n          attention_chananels: 32\n          embedding_patch_sizes: 7\n          embedding_strides: 4\n          num_attention_heads: 1\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 4\n          attention_chananels: 64\n          embedding_patch_sizes: 3\n          embedding_strides: 2\n          num_attention_heads: 2\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 2\n          attention_chananels: 160\n          embedding_patch_sizes: 3\n          embedding_strides: 2\n          num_attention_heads: 5\n        -\n          num_blocks: 2\n          sequence_reduction_ratio: 1\n          attention_chananels: 256\n          embedding_patch_sizes: 3\n          embedding_strides: 2\n          num_attention_heads: 8\n</code></pre>"},{"location":"models/backbones/mixtransformer/#related-links","title":"Related links","text":"<ul> <li><code>huggingface/transformers</code></li> </ul>"},{"location":"models/backbones/mobilenetv3/","title":"MobileNetV3","text":"<p>MobileNetV3 backbone based on Searching for MobileNetV3.</p> <p>We provide a configuration that allows users to define each inverted residual block in MobileNetV3 individually. You can specify in a list format the number and form of each inverted residual block for every stage. Using this, we provide both MobileNetV3-small and MobileNetV3-large.</p>"},{"location":"models/backbones/mobilenetv3/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mobilenetv3\" to use <code>MobileNetV3</code> backbone. <code>stage_params[n].in_channels</code> (list[int]) Input dimensions for the inverted residual blocks in the stage. <code>stage_params[n].kernel_sizes</code> (list[int]) Convolution kernel sizes for the inverted residual blocks in the stage. <code>stage_params[n].expanded_channels</code> (list[int]) Expanded dimensions for the inverted residual blocks in the stage. <code>stage_params[n].out_channels</code> (list[int]) Output dimensions for the inverted residual blocks in the stage. <code>stage_params[n].use_se</code> (list[bool]) Flags that determine whether to use squeeze-and-excitation blocks for the inverted residual blocks in the stage. <code>stage_params[n].activation</code> (list[str]) Type of activation functions for the inverted residual blocks in the stage. Supporting activation functions are described in [here] <code>stage_params[n].stride</code> (list[int]) Stride values for the inverted residual blocks included in the stage. <code>stage_params[n].dilation</code> (list[int]) Dilation values for the inverted residual blocks in the stage."},{"location":"models/backbones/mobilenetv3/#model-configuration-examples","title":"Model configuration examples","text":"MobileNetV3-small <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv3\n      params: ~\n      stage_params:\n        -\n          in_channels: [16]\n          kernel_sizes: [3]\n          expanded_channels: [16]\n          out_channels: [16]\n          use_se: [True]\n          act_type: [\"relu\"]\n          stride: [2]\n        -\n          in_channels: [16, 24]\n          kernel_sizes: [3, 3]\n          expanded_channels: [72, 88]\n          out_channels: [24, 24]\n          use_se: [False, False]\n          act_type: [\"relu\", \"relu\"]\n          stride: [2, 1]\n        -\n          in_channels: [24, 40, 40, 40, 48]\n          kernel_sizes: [5, 5, 5, 5, 5]\n          expanded_channels: [96, 240, 240, 120, 144]\n          out_channels: [40, 40, 40, 48, 48]\n          use_se: [True, True, True, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1, 1, 1]\n        -\n          in_channels: [48, 96, 96]\n          kernel_sizes: [5, 5, 5]\n          expanded_channels: [288, 576, 576]\n          out_channels: [96, 96, 96]\n          use_se: [True, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1]\n</code></pre> MobileNetV3-large <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv3\n      params: ~\n      stage_params:\n        -\n          in_channels: [16, 16, 24]\n          kernel_sizes: [3, 3, 3]\n          expanded_channels: [16, 64, 72]\n          out_channels: [16, 24, 24]\n          use_se: [False, False, False]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          stride: [1, 2, 1]\n        - \n          in_channels: [24, 40, 40]\n          kernel_sizes: [5, 5, 5]\n          expanded_channels: [72, 120, 120]\n          out_channels: [40, 40, 40]\n          use_se: [True, True, True]\n          act_type: [\"relu\", \"relu\", \"relu\"]\n          stride: [2, 1, 1]\n        -\n          in_channels: [40, 80, 80, 80, 80, 112]\n          kernel_sizes: [3, 3, 3, 3, 3, 3]\n          expanded_channels: [240, 200, 184, 184, 480, 672]\n          out_channels: [80, 80, 80, 80, 112, 112]\n          use_se: [False, False, False, False, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1, 1, 1, 1]\n        -\n          in_channels: [112, 160, 160]\n          kernel_sizes: [5, 5, 5]\n          expanded_channels: [672, 960, 960]\n          out_channels: [160, 160, 160]\n          use_se: [True, True, True]\n          act_type: [\"hard_swish\", \"hard_swish\", \"hard_swish\"]\n          stride: [2, 1, 1]\n</code></pre>"},{"location":"models/backbones/mobilenetv3/#related-links","title":"Related links","text":"<ul> <li><code>pytorch/vision</code></li> </ul>"},{"location":"models/backbones/mobilenetv4/","title":"MobileNetV4","text":"<p>MobileNetV4 backbone based on MobileNetV4 -- Universal Models for the Mobile Ecosystem.</p>"},{"location":"models/backbones/mobilenetv4/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mobilenetv4\" to use <code>MobileNetV4</code> backbone. <code>stage_params[n] case1: Conv2D</code> (list) Build <code>Conv2D</code> layer under following format: <code>['conv', out_channels, kernel_size, stride]</code>. <code>stage_params[n] case2: FusedIB</code> (list) Build <code>FusedIB</code> block under following format: <code>['fi', out_channels, hidden_channels, kernel_size, stride]</code>. <code>stage_params[n] case3: UniversalInvertedResidualBlock</code> (list) Build <code>UniversalInvertedResidualBlock</code> block under following format: <code>['uir', out_channels, hidden_channels, extra_dw, extra_dw_kernel_size, middle_dw, middle_dw_kernel_size, stride]</code>. <code>stage_params[n] case4: MobileMultiQueryAttention2D</code> (list) Build <code>MobileMultiQueryAttention2D</code> block under following format: <code>['mmqa', out_channels, attention_channel, num_attention_heads, query_pooling_stride, key_val_downsample, key_val_downsample_kernel_size, key_val_downsample_stride, stride]</code>."},{"location":"models/backbones/mobilenetv4/#model-configuration-examples","title":"Model configuration examples","text":"MobileNetV4-conv-small <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv4\n      params:\n        stem_out_channel: 32\n        stem_kernel_size: 3\n        stem_stride: 2\n        final_conv_out_channel: 960\n        final_conv_kernel_size: 1\n        final_conv_stride: 1\n        norm_type: batch_norm\n        act_type: relu\n        return_stage_idx: ~\n        layer_scale: 0.1\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # FusedIB: ['fi', out_channels, hidden_channels, kernel_size, stride]\n        # UniversalInvertedResidualBlock: ['uir', out_channels, hidden_channels, extra_dw, extra_dw_kernel_size, middle_dw, middle_dw_kernel_size, stride]\n        # MobileMultiQueryAttention2D: ['mmqa', out_channels, attention_channel, num_attention_heads, query_pooling_stride, key_val_downsample, key_val_downsample_kernel_size, key_val_downsample_stride, stride]\n        - \n          - ['conv', 32, 3, 2]\n          - ['conv', 32, 1, 1]\n        - \n          - ['conv', 96, 3, 2]\n          - ['conv', 64, 1, 1]\n        - \n          - ['uir', 96, 192, True, 5, True, 5, 2]\n          - ['uir', 96, 192, False, ~, True, 3, 1]\n          - ['uir', 96, 192, False, ~, True, 3, 1]\n          - ['uir', 96, 192, False, ~, True, 3, 1]\n          - ['uir', 96, 192, False, ~, True, 3, 1]\n          - ['uir', 96, 384, True, 3, False, ~, 1]\n        - \n          - ['uir', 128, 576, True, 3, True, 3, 2]\n          - ['uir', 128, 512, True, 5, True, 5, 1]\n          - ['uir', 128, 512, False, ~, True, 5, 1]\n          - ['uir', 128, 384, False, ~, True, 5, 1]\n          - ['uir', 128, 512, False, ~, True, 3, 1]\n          - ['uir', 128, 512, False, ~, True, 3, 1]\n</code></pre> MobileNetV4-conv-medium <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv4\n      params:\n        stem_out_channel: 32\n        stem_kernel_size: 3\n        stem_stride: 2\n        final_conv_out_channel: 960\n        final_conv_kernel_size: 1\n        final_conv_stride: 1\n        norm_type: batch_norm\n        act_type: relu\n        return_stage_idx: ~\n        layer_scale: ~\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # FusedIB: ['fi', out_channels, hidden_channels, kernel_size, stride]\n        # UniversalInvertedResidualBlock: ['uir', out_channels, hidden_channels, extra_dw, extra_dw_kernel_size, middle_dw, middle_dw_kernel_size, stride]\n        # MobileMultiQueryAttention2D: ['mmqa', out_channels, attention_channel, num_attention_heads, query_pooling_stride, key_val_downsample, key_val_downsample_kernel_size, key_val_downsample_stride, stride]\n        - \n          - ['fi', 48, 128, 3, 2]\n        - \n          - ['uir', 80, 192, True, 3, True, 5, 2]\n          - ['uir', 80, 160, True, 3, True, 3, 1]\n        - \n          - ['uir', 160, 480, True, 3, True, 5, 2]\n          - ['uir', 160, 640, True, 3, True, 3, 1]\n          - ['uir', 160, 640, True, 3, True, 3, 1]\n          - ['uir', 160, 640, True, 3, True, 5, 1]\n          - ['uir', 160, 640, True, 3, True, 3, 1]\n          - ['uir', 160, 640, True, 3, False, ~, 1]\n          - ['uir', 160, 320, False, ~, False, ~, 1]\n          - ['uir', 160, 640, True, 3, False, ~, 1]\n        - \n          - ['uir', 256, 960, True, 5, True, 5, 2]\n          - ['uir', 256, 1024, True, 5, True, 5, 1]\n          - ['uir', 256, 1024, True, 3, True, 5, 1]\n          - ['uir', 256, 1024, True, 3, True, 5, 1]\n          - ['uir', 256, 1024, False, ~, False, ~, 1]\n          - ['uir', 256, 1024, True, 3, False, ~, 1]\n          - ['uir', 256, 512, True, 3, True, 5, 1]\n          - ['uir', 256, 1024, True, 5, True, 5, 1]\n          - ['uir', 256, 1024, False, ~, False, ~, 1]\n          - ['uir', 256, 1024, False, ~, False, ~, 1]\n          - ['uir', 256, 512, True, 5, False, ~, 1]\n</code></pre> MobileNetV4-conv-large <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv4\n      params:\n        stem_out_channel: 24\n        stem_kernel_size: 3\n        stem_stride: 2\n        final_conv_out_channel: 960\n        final_conv_kernel_size: 1\n        final_conv_stride: 1\n        norm_type: batch_norm\n        act_type: relu\n        return_stage_idx: ~\n        layer_scale: ~\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # FusedIB: ['fi', out_channels, hidden_channels, kernel_size, stride]\n        # UniversalInvertedResidualBlock: ['uir', out_channels, hidden_channels, extra_dw, extra_dw_kernel_size, middle_dw, middle_dw_kernel_size, stride]\n        # MobileMultiQueryAttention2D: ['mmqa', out_channels, attention_channel, num_attention_heads, query_pooling_stride, key_val_downsample, key_val_downsample_kernel_size, key_val_downsample_stride, stride]\n        - \n          - ['fi', 48, 96, 3, 2]\n        - \n          - ['uir', 96, 192, True, 3, True, 5, 2]\n          - ['uir', 96, 384, True, 3, True, 3, 1]\n        - \n          - ['uir', 192, 384, True, 3, True, 5, 2]\n          - ['uir', 192, 768, True, 3, True, 3, 1]\n          - ['uir', 192, 768, True, 3, True, 3, 1]\n          - ['uir', 192, 768, True, 3, True, 3, 1]\n          - ['uir', 192, 768, True, 3, True, 5, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['uir', 192, 768, True, 3, False, ~, 1]\n        - \n          - ['uir', 512, 768, True, 5, True, 5, 2]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, True, 3, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, True, 3, 1]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n</code></pre> MobileNetV4-hybrid-medium <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv4\n      params:\n        stem_out_channel: 32\n        stem_kernel_size: 3\n        stem_stride: 2\n        final_conv_out_channel: 960\n        final_conv_kernel_size: 1\n        final_conv_stride: 1\n        norm_type: batch_norm\n        act_type: relu\n        return_stage_idx: ~\n        layer_scale: 0.1\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # FusedIB: ['fi', out_channels, hidden_channels, kernel_size, stride]\n        # UniversalInvertedResidualBlock: ['uir', out_channels, hidden_channels, extra_dw, extra_dw_kernel_size, middle_dw, middle_dw_kernel_size, stride]\n        # MobileMultiQueryAttention2D: ['mmqa', out_channels, attention_channel, num_attention_heads, query_pooling_stride, key_val_downsample, key_val_downsample_kernel_size, key_val_downsample_stride, stride]\n        - \n          - ['fi', 48, 128, 3, 2]\n        - \n          - ['uir', 80, 192, True, 3, True, 5, 2]\n          - ['uir', 80, 160, True, 3, True, 3, 1]\n        - \n          - ['uir', 160, 480, True, 3, True, 5, 2]\n          - ['uir', 160, 320, False, ~, False, ~, 1]\n          - ['uir', 160, 640, True, 3, True, 3, 1]\n          - ['uir', 160, 640, True, 3, True, 5, 1]\n          - ['mmqa', 160, 256, 4, ~, True, 3, 2, 1]\n          - ['uir', 160, 640, True, 3, True, 3, 1]\n          - ['mmqa', 160, 256, 4, ~, True, 3, 2, 1]\n          - ['uir', 160, 640, True, 3, False, ~, 1]\n          - ['mmqa', 160, 256, 4, ~, True, 3, 2, 1]\n          - ['uir', 160, 640, True, 3, True, 3, 1]\n          - ['mmqa', 160, 256, 4, ~, True, 3, 2, 1]\n          - ['uir', 160, 640, True, 3, False, ~, 1]\n        - \n          - ['uir', 256, 960, True, 5, True, 5, 2]\n          - ['uir', 256, 1024, True, 5, True, 5, 1]\n          - ['uir', 256, 1024, True, 3, True, 5, 1]\n          - ['uir', 256, 1024, True, 3, True, 5, 1]\n          - ['uir', 256, 512, False, ~, False, ~, 1]\n          - ['uir', 256, 512, True, 3, True, 5, 1]\n          - ['uir', 256, 512, False, ~, False, ~, 1]\n          - ['uir', 256, 1024, False, ~, False, ~, 1]\n          - ['mmqa', 256, 256, 4, ~, False, ~, ~, 1]\n          - ['uir', 256, 1024, True, 3, False, ~, 1]\n          - ['mmqa', 256, 256, 4, ~, False, ~, ~, 1]\n          - ['uir', 256, 1024, True, 5, True, 5, 1]\n          - ['mmqa', 256, 256, 4, ~, False, ~, ~, 1]\n          - ['uir', 256, 1024, True, 5, False, ~, 1]\n          - ['mmqa', 256, 256, 4, ~, False, ~, ~, 1]\n          - ['uir', 256, 1024, True, 5, False, ~, 1]\n</code></pre> MobileNetV4-hybrid-large <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilenetv4\n      params:\n        stem_out_channel: 24\n        stem_kernel_size: 3\n        stem_stride: 2\n        final_conv_out_channel: 960\n        final_conv_kernel_size: 1\n        final_conv_stride: 1\n        norm_type: batch_norm\n        act_type: gelu\n        return_stage_idx: ~\n        layer_scale: 0.1\n      stage_params:\n        # Conv2D: ['conv', out_channels, kernel_size, stride]\n        # FusedIB: ['fi', out_channels, hidden_channels, kernel_size, stride]\n        # UniversalInvertedResidualBlock: ['uir', out_channels, hidden_channels, extra_dw, extra_dw_kernel_size, middle_dw, middle_dw_kernel_size, stride]\n        # MobileMultiQueryAttention2D: ['mmqa', out_channels, attention_channel, num_attention_heads, query_pooling_stride, key_val_downsample, key_val_downsample_kernel_size, key_val_downsample_stride, stride]\n        - \n          - ['fi', 48, 96, 3, 2]\n        - \n          - ['uir', 96, 192, True, 3, True, 5, 2]\n          - ['uir', 96, 384, True, 3, True, 3, 1]\n        - \n          - ['uir', 192, 384, True, 3, True, 5, 2]\n          - ['uir', 192, 768, True, 3, True, 3, 1]\n          - ['uir', 192, 768, True, 3, True, 3, 1]\n          - ['uir', 192, 768, True, 3, True, 3, 1]\n          - ['uir', 192, 768, True, 3, True, 5, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['mmqa', 192, 384, 8, ~, True, 3, 2, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['mmqa', 192, 384, 8, ~, True, 3, 2, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['mmqa', 192, 384, 8, ~, True, 3, 2, 1]\n          - ['uir', 192, 768, True, 5, True, 3, 1]\n          - ['mmqa', 192, 384, 8, ~, True, 3, 2, 1]\n          - ['uir', 192, 768, True, 3, False, ~, 1]\n        - \n          - ['uir', 512, 768, True, 5, True, 5, 2]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, True, 3, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['uir', 512, 2048, True, 5, True, 3, 1]\n          - ['uir', 512, 2048, True, 5, True, 5, 1]\n          - ['mmqa', 512, 512, 8, ~, False, ~, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['mmqa', 512, 512, 8, ~, False, ~, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['mmqa', 512, 512, 8, ~, False, ~, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n          - ['mmqa', 512, 512, 8, ~, False, ~, ~, 1]\n          - ['uir', 512, 2048, True, 5, False, ~, 1]\n</code></pre>"},{"location":"models/backbones/mobilenetv4/#related-links","title":"Related links","text":"<ul> <li><code>timm</code></li> </ul>"},{"location":"models/backbones/mobilevit/","title":"MobileViT","text":"<p>MobileViT backbone based on MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer.</p> <p>MobileViT was introduced by combining inverted residual blocks with transformer-based MobileViT blocks. In line with this, it is possible to select between inverted residual blocks (as mv2) and MobileViT models for each stage of the backbone with detailed configurations according to each block type.</p>"},{"location":"models/backbones/mobilevit/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"mobilevit\" to use MobileViT backbone. <code>params.patch_size</code> (int) Patch size for MobileViT blocks. <code>params.num_attention_heads</code> (int) The number of heads in the multi-head attention. <code>params.attention_dropout_prob</code> (float) Dropout probability in the attention. <code>params.ffn_dropout_prob</code> (float) Dropout probability in the feed-forward network inside of the attention block. <code>params.output_expansion_ratio</code> (int) Expansion ratio for computing output dimension of the model. If expanded dimension is bigger than 960, it is set to 960. <code>params.use_fusion_layer</code> (bool) Whether to use fusion layer for MobileViT blocks. <code>stage_params[n].block_type</code> (str) Determines which block to use, \"mv2\" or \"mobilevit\". <code>stage_params[n].out_channels</code> (int) Output dimension of the block. <code>stage_params[n].num_blocks</code> (int) The number of blocks in the stage. Note that if <code>block_type</code> is <code>mobilevit</code>, an extra inverted residual block is added before MobileViT blocks. <code>stage_params[n].stride</code> (int) Stride value for the block. <code>stage_params[n].attention_channels</code> (int) Dimension for the attention block. If is used only <code>block_type</code> is \"mobilevit\". <code>stage_params[n].ffn_intermediate_channels</code> (int) Intermediate dimension for the feed forward network inside of the attention block. <code>stage_params[n].dilate</code> (bool) Whether to replace stride as dilated convolution. It is used only <code>block_type</code> is <code>mobilevit</code>. <code>stage_params[n].ir_expansion_ratio</code> (int) Dimension expansion ratio for inverted residual block."},{"location":"models/backbones/mobilevit/#model-configuration-examples","title":"Model configuration examples","text":"MobileViT-xxs <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilevit\n      params:\n        patch_size: 2\n        num_attention_heads: 4  # num_heads\n        attention_dropout_prob: 0.1\n        ffn_dropout_prob: 0.0\n        output_expansion_ratio: 4\n        use_fusion_layer: True\n      stage_params:\n        -\n          block_type: 'mv2'\n          out_channels: 16\n          num_blocks: 1\n          stride: 1\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mv2'\n          out_channels: 24\n          num_blocks: 3\n          stride: 2\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mobilevit'\n          out_channels: 48\n          num_blocks: 2\n          stride: 2\n          attention_channels: 64\n          ffn_intermediate_channels: 128\n          dilate: False\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mobilevit'\n          out_channels: 64\n          num_blocks: 4\n          stride: 2\n          attention_channels: 80\n          ffn_intermediate_channels: 160\n          dilate: False\n          ir_expansion_ratio: 2\n        -\n          block_type: 'mobilevit'\n          out_channels: 80\n          num_blocks: 3\n          stride: 2\n          attention_channels: 96\n          ffn_intermediate_channels: 192\n          dilate: False\n          ir_expansion_ratio: 2\n</code></pre> MobileViT-xs <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilevit\n      params:\n        patch_size: 2\n        num_attention_heads: 4  # num_heads\n        attention_dropout_prob: 0.1\n        ffn_dropout_prob: 0.0\n        output_expansion_ratio: 4\n        use_fusion_layer: True\n      stage_params:\n        -\n          block_type: 'mv2'\n          out_channels: 32\n          num_blocks: 1\n          stride: 1\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mv2'\n          out_channels: 48\n          num_blocks: 3\n          stride: 2\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 64\n          num_blocks: 2\n          stride: 2\n          attention_channels: 96\n          ffn_intermediate_channels: 192\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 80\n          num_blocks: 4\n          stride: 2\n          attention_channels: 120\n          ffn_intermediate_channels: 240\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 96\n          num_blocks: 3\n          stride: 2\n          attention_channels: 144\n          ffn_intermediate_channels: 288\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n</code></pre> MobileViT-s <pre><code>model:\n  architecture:\n    backbone:\n      name: mobilevit\n      params:\n        patch_size: 2\n        num_attention_heads: 4  # num_heads\n        attention_dropout_prob: 0.1\n        ffn_dropout_prob: 0.0\n        output_expansion_ratio: 4\n        use_fusion_layer: True\n      stage_params:\n        -\n          block_type: 'mv2'\n          out_channels: 32\n          num_blocks: 1\n          stride: 1\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mv2'\n          out_channels: 64\n          num_blocks: 3\n          stride: 2\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 96\n          num_blocks: 2\n          stride: 2\n          attention_channels: 144\n          ffn_intermediate_channels: 288\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 128\n          num_blocks: 4\n          stride: 2\n          attention_channels: 192\n          ffn_intermediate_channels: 384\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n        -\n          block_type: 'mobilevit'\n          out_channels: 160\n          num_blocks: 3\n          stride: 2\n          attention_channels: 240\n          ffn_intermediate_channels: 480\n          dilate: False\n          ir_expansion_ratio: 4  # [mv2_exp_mult] * 4\n</code></pre>"},{"location":"models/backbones/mobilevit/#related-links","title":"Related links","text":"<ul> <li><code>apple/ml-cvnets</code></li> </ul>"},{"location":"models/backbones/resnet/","title":"ResNet","text":"<p>ResNet backbone based on Deep Residual Learning for Image Recognition.</p> <p>You can flexibly choose between basicblock and bottleneck as the building blocks for the ResNet architecture. And, you can also freely determine the number of stages and the repetition of blocks within the model. This flexibility supports the creation of various ResNet models, e.g. ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152. Also this supports adjusting the number of stages and blocks for your specific requirements.</p>"},{"location":"models/backbones/resnet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"resnet\" to use <code>ResNet</code> backbone. <code>params.block_type</code> (str) Key value that determines which block to use, \"basicblock\" or \"bottleneck\". <code>params.norm_type</code> (str) Type of normalization layer. Supporting normalization layers are described in [here]. <code>stage_params[n].channels</code> (int) The dimension of the first convolution layer in each block. <code>stage_params[n].num_blocks</code> (int) The number of blocks in the stage. <code>stage_params[n].replace_stride_with_dilation</code> (bool) Flag that determines whether to replace stride step with dilated convolution."},{"location":"models/backbones/resnet/#model-configuration-examples","title":"Model configuration examples","text":"ResNet18 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: basicblock\n        norm_layer: batch_norm\n      stage_params:\n        - \n          channels: 64\n          layers: 2\n        - \n          channels: 128\n          layers: 2\n          replace_stride_with_dilation: False\n        - \n          channels: 256\n          layers: 2\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 2\n          replace_stride_with_dilation: False\n</code></pre> ResNet34 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: basicblock\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 4\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 6\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre> ResNet50 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: bottleneck\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 4\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 6\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre> ResNet101 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: bottleneck\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 4\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 23\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre> ResNet152 <pre><code>model:\n  architecture:\n    backbone:\n      name: resnet\n      params:\n        block: bottleneck\n        norm_layer: batch_norm\n      stage_params:\n        - \n          plane: 64\n          layers: 3\n        - \n          plane: 128\n          layers: 8\n          replace_stride_with_dilation: False\n        - \n          plane: 256\n          layers: 36\n          replace_stride_with_dilation: False\n        - \n          plane: 512\n          layers: 3\n          replace_stride_with_dilation: False\n</code></pre>"},{"location":"models/backbones/resnet/#related-links","title":"Related links","text":"<ul> <li><code>pytorch/vision</code></li> </ul>"},{"location":"models/backbones/vit/","title":"ViT","text":"<p>ViT backbone based on An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</p> <p>ViT (Vision Transformer) does not have a stage configuration and therefore does not support compatibility with neck modules. Currently, it only supports the FC head. When using the ViT model for classification tasks, users can decide whether to use a classification token. Additionally, users can flexibly configure the settings of the transformer encoder.</p>"},{"location":"models/backbones/vit/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"vit\" to use <code>ViT</code> backbone. <code>params.patch_size</code> (int) Size of the image patch to be treated as a single embedding. <code>params.attention_channels</code> (int) Dimension for the encoder. <code>params.num_blocks</code> (int) The number of self-attention blocks in the encoder. <code>params.num_attention_heads</code> (int) The number of heads in the multi-head attention. <code>params.attention_dropout_prob</code> (float) Dropout probability in the attention block. <code>params.ffn_intermediate_channels</code> (int) Intermediate dimension of the feed-forward network inside the attention block. <code>params.ffn_dropout_prob</code> (float) Dropout probability in the feed-forward network inside the attention block. <code>params.use_cls_token</code> (bool) Whether to use the classification token. <code>params.vocab_size</code> (int) Maximum token length for positional encoding."},{"location":"models/backbones/vit/#model-configuration-examples","title":"Model configuration examples","text":"ViT-tiny <pre><code>model:\n  architecture:\n    backbone:\n      name: vit\n      params:\n        patch_size: 16\n        attention_channels: 192\n        num_blocks: 12\n        num_attention_heads: 3\n        attention_dropout_prob: 0.0\n        ffn_intermediate_channels: 768  # hidden_size * 4\n        ffn_dropout_prob: 0.1\n        use_cls_token: True\n        vocab_size: 1000\n      stage_params: ~\n</code></pre> ViT-small <pre><code>model:\n  architecture:\n    backbone:\n      name: vit\n      params:\n        patch_size: 16\n        attention_channels: 384\n        num_blocks: 12\n        num_attention_heads: 6\n        attention_dropout_prob: 0.0\n        ffn_intermediate_channels: 1536  # hidden_size * 4\n        ffn_dropout_prob: 0.0\n        use_cls_token: True\n        vocab_size: 1000\n      stage_params: ~\n</code></pre>"},{"location":"models/backbones/vit/#related-links","title":"Related links","text":"<ul> <li><code>apple/ml-cvnets</code></li> </ul>"},{"location":"models/fullmodels/pidnet/","title":"PIDNet","text":"<p>PIDNet model based on PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers.</p>"},{"location":"models/fullmodels/pidnet/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"pidnet\" to use PIDNet model. <code>m</code> (int) Residual block repetetion count for 1, 2 stage of I branch, and 3, 4 stage of P branch. <code>n</code> (int) Residual block repetition count for 3, 4 stage of I branch <code>channels</code> (int) Base dimension of the overall model except ppm and head module. <code>ppm_channels</code> (int) Dimension of the ppm module. <code>head_channels</code> (int) Dimension of the head module. PIDNet-s <pre><code>model:\n  architecture:\n    full:\n      name: pidnet\n      m: 2\n      n: 3\n      channels: 32\n      ppm_channels: 96\n      head_channels: 128\n</code></pre> PIDNet-m <pre><code>model:\n  architecture:\n    full:\n      name: pidnet\n      m: 2\n      n: 3\n      channels: 64\n      ppm_channels: 96\n      head_channels: 128\n</code></pre> PIDNet-l <pre><code>model:\n  architecture:\n    full:\n      name: pidnet\n      m: 3\n      n: 4\n      channels: 64\n      ppm_channels: 112\n      head_channels: 256\n</code></pre>"},{"location":"models/fullmodels/pidnet/#related-links","title":"Related links","text":"<ul> <li><code>XuJiacong/PIDNet</code></li> </ul>"},{"location":"models/heads/allmlpdecoder/","title":"AllMLPDecoder","text":"<p>All-MLP decoder based on SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.</p> <p>We provide the AllMLP Decoder, the head of SegFormer, as a freely usable head module. AllMLP Decoder takes intermediate features from previous backbone or neck module and outputs a segmentation map of the target size.</p>"},{"location":"models/heads/allmlpdecoder/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"all_mlp_decoder\" to use <code>AllMLPDecoder</code> head. <code>params.intermediate_channels</code> (int) Intermediate feature dimension of the decoder. <code>params.classifier_dropout_prob</code> (float) Dropout probability of classifier."},{"location":"models/heads/allmlpdecoder/#model-configuration-example","title":"Model configuration example","text":"AllMLP decoder <pre><code>model:\n  architecture:\n    head:\n      name: all_mlp_decoder\n      params:\n        intermediate_channels: 256\n        classifier_dropout_prob: 0.\n</code></pre>"},{"location":"models/heads/allmlpdecoder/#related-links","title":"Related links","text":""},{"location":"models/heads/anchordecoupledhead/","title":"AnchorDecoupledHead","text":"<p>Decoupled detection head with anchors based on Focal Loss for Dense Object Detection</p> <p>We have named the detection head of RetinaNet as AnchorDecoupledHead to represent it in a more general term. AnchorDecoupledHead consists of a box regression head and a classification head for the given intermediate features, predicting detection boxes for the anchors at each feature's pixel location. Additionally, we provide the option to adjust the number of convolution layers used in the heads through the <code>num_layers</code> value, it is equivalent with RetinaNet when set to 4.</p>"},{"location":"models/heads/anchordecoupledhead/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"retinanet_head\" to use <code>RetinaNetHead</code> head. <code>params.anchor_sizes</code> (list[list[int]]) Default anchor sizes for each intermediate feature. <code>params.aspect_ratios</code> (list[float]) List of aspect ratio for each anchor. <code>params.num_layers</code> (int) The number of convolution layers of regression and classification head. <code>params.norm_layer</code> (str) Normalization type for the head."},{"location":"models/heads/anchordecoupledhead/#model-configuration-example","title":"Model configuration example","text":"Anchor-based decoupled detection head <pre><code>model:\n  architecture:\n    head:\n      name: anchor_decoupled_head\n      params:\n        anchor_sizes: [[32,], [64,], [128,], [256,]]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        num_layers: 1\n        norm_type: batch_norm \n</code></pre>"},{"location":"models/heads/anchordecoupledhead/#related-links","title":"Related links","text":"<ul> <li><code>pytorch/vision</code></li> </ul>"},{"location":"models/heads/anchorfreedecoupledhead/","title":"AnchorFreeDecoupledHead","text":"<p>Anchor-free decoupled detection head based on YOLOX: Exceeding YOLO Series in 2021.</p> <p>We provide the head of YOLOX as AnchorFreeDecoupledHead. There are no differnece with the original model, and currently, it is set to pass non-maximum suppression function.</p>"},{"location":"models/heads/anchorfreedecoupledhead/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"yolox_head\" to use <code>YOLOX</code> head. <code>params.act_type</code> (float) Activation function for the head. <code>params.depthwise</code> (bool) Whether to enable depthwise convolution for the head."},{"location":"models/heads/anchorfreedecoupledhead/#model-configuration-example","title":"Model configuration example","text":"Anchor-free decoupled detection head <pre><code>model:\n  architecture:\n    head:\n      name: anchor_free_decoupled_head\n      params:\n        depthwise: False\n        act_type: \"silu\" \n</code></pre>"},{"location":"models/heads/anchorfreedecoupledhead/#related-links","title":"Related links","text":"<ul> <li><code>Megvii-BaseDetection/YOLOX</code></li> </ul>"},{"location":"models/heads/fc/","title":"FC","text":"<p>Fully connected layer head for classification. You can adjust the number of layers and channel sizes. Channel of last layer is always same with the number of classes.</p>"},{"location":"models/heads/fc/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"fc\" to use <code>FC</code> head. <code>params.num_layers</code> (int) The number of fully connected layers. Channel of last layer is same with the number of classes. <code>params.intermediate_channels</code> (int) Dimension of intermediate fully connected layers. This can be ignored if <code>num_layer</code> is 1. <code>params.act_type</code> (str) Activation function for intermediate fully connected layers. This can be ignored if <code>num_layer</code> is 1. <code>params.dropout_prob</code> (float) Dropout probability before the last classifier layer."},{"location":"models/heads/fc/#model-configuration-example","title":"Model configuration example","text":"2-layer fully connected layer classifier <pre><code>model:\n  architecture:\n    head:\n      name: fc\n      params:\n        num_layers: 2\n        intermediate_channels: 1024\n        act_type: hard_swish\n        dropout_prob: 0.2\n</code></pre>"},{"location":"models/heads/fc/#related-links","title":"Related links","text":""},{"location":"models/heads/rtdetrhead/","title":"RT-DETR Head","text":"<p>RT-DETR detection head based on DETRs Beat YOLOs on Real-time Object Detection.</p> <p>We provide the head of RT-DETR as <code>rtdetr_head</code>. </p>"},{"location":"models/heads/rtdetrhead/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"rtdetr_head\" to use <code>RT-DETR Head</code> head. <code>params.hidden_dim</code> (int) Hidden dimension size, default is 256 according to paper's Appendix Table A <code>params.num_attention_heads</code> (int) Number of attention heads, default is 8 according to paper's Appendix Table A <code>params.num_levels</code> (int) Number of feature levels used, default is 3 according to paper's Section 4.1 <code>params.num_queries</code> (int) Number of object queries, default is 300 according to paper's Section 4.1 and Appendix Table A <code>params.eps</code> (float) Small constant for numerical stability, default is 1e-2 <code>params.num_decoder_layers</code> (int) Number of decoder layers. <code>params.position_embed_type</code> (str) Type of position embedding used ['sine', 'learned']. <code>params.num_decoder_points</code> (int) Number of decoder reference points, default is 4 according to paper's Appendix Table A. <code>params.dim_feedforward</code> (int) Feedforward network dimension, default is 1024 according to paper's Appendix Table A. <code>params.dropout</code> (float) Dropout rate in layers. <code>params.act_type</code> (str) Activation function type. <code>params.num_denoising</code> (int) Number of denoising queries. <code>params.label_noise_ratio</code> (float) Label noise ratio for denoising training, default is 0.5 according to paper's Appendix Table A. <code>params.use_aux_loss</code> (bool) Whether to use auxiliary loss when training. The paper mentions using auxiliary prediction heads in Section 4.1."},{"location":"models/heads/rtdetrhead/#model-configuration-example","title":"Model configuration example","text":"RT-DETR head <pre><code>model:\n  architecture:\n    head:\n      name: rtdetr_head\n    params:\n      hidden_dim: 256\n      num_attention_heads: 8\n      num_levels: 3\n      num_queries: 300\n      eps: 1e-2\n      num_decoder_layers: 3\n      eval_spatial_size: ~\n      position_embed_type: sine\n      num_decoder_points: 4\n      dim_feedforward: 1024\n      dropout: 0.0\n      act_type: relu\n      num_denoising: 100\n      label_noise_ratio: 0.5\n      use_aux_loss: true\n</code></pre>"},{"location":"models/heads/rtdetrhead/#related-links","title":"Related links","text":"<ul> <li>DETRs Beat YOLOs on Real-time Object Detection </li> <li>lyuwenyu/RT-DETR</li> </ul>"},{"location":"models/heads/rtmcc/","title":"RTMCC","text":"<p>RTMCC head based on RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose.</p>"},{"location":"models/heads/rtmcc/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"rtmcc\" to use <code>RTMCC</code> head. <code>params.conv_kernel</code> (int) Kernel size of convolution layer. <code>params.attention_channels</code> (int) Dimension of gated attention unit. <code>params.attention_act_type</code> (str) Activation type of gated attention unit. <code>params.attention_pos_enc</code> (bool) Whether to use rotary position embedding for gated attention unit. <code>params.s</code> (int) Self attention feature dimension of gated attention unit. <code>params.expansion_factor</code> (int) Expansion factor of gated attention unit. <code>params.dropout_rate</code> (float) Dropout rate of gated attention unit. <code>params.drop_path</code> (float) Drop path rate of gated attention unit. <code>params.use_rel_bias</code> (bool) Whether to use relative bias for gated attention unit. <code>params.simcc_split_ratio</code> (float) Split ratio of pixels. <code>params.target_size</code> (list) Original input image size. <code>params.backbone_stride</code> (int) Stride of input feature from original image produced by backbone."},{"location":"models/heads/rtmcc/#model-configuration-example","title":"Model configuration example","text":"RTMCC head <pre><code>model:\n  architecture:\n    head:\n    name: rtmcc\n    params:\n      conv_kernel: 7\n      attention_channels: 256\n      attention_act_type: 'silu'\n      attention_pos_enc: False\n      s: 128\n      expansion_factor: 2\n      dropout_rate: 0.\n      drop_path: 0.\n      use_rel_bias: False\n      simcc_split_ratio: 2.\n      target_size: [256, 256]\n      backbone_stride: 32\n</code></pre>"},{"location":"models/heads/rtmcc/#related-links","title":"Related links","text":"<ul> <li><code>RTMPose</code></li> </ul>"},{"location":"models/layers/activations/","title":"Activations","text":"<p>Choosing Activation functions is a key part of neural network design, since they determines the output of nodes in the network. They introduce non-linear properties to the network, enabling it to learn complex data patterns and make more sophisticated predictions.</p> <p>To enable diverse model designs, NetsPresso Trainer supports various activation modules based on the pytorch library.</p>"},{"location":"models/layers/activations/#supporting-activation-functions","title":"Supporting activation functions","text":"<p>The currently supported activation functions in NetsPresso Trainer are as follows.</p>"},{"location":"models/layers/activations/#relu","title":"ReLU","text":"<ul> <li>This can be applied by giving <code>'relu'</code> keyword</li> <li>ReLU activation follows the torch.nn.ReLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#prelu","title":"PReLU","text":"<ul> <li>This can be applied by giving <code>'prelu'</code> keyword</li> <li>PReLU activation follows the torch.nn.PReLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li>This can be applied by giving <code>'leaky_relu'</code> keyword</li> <li>Leaky ReLU activation follows the torch.nn.LeakyReLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#gelu","title":"GELU","text":"<ul> <li>This can be applied by giving <code>'gelu'</code> keyword</li> <li>GELU activation follows the torch.nn.GELU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#silu","title":"SiLU","text":"<ul> <li>This can be applied by giving <code>'silu'</code> or <code>'swish'</code>keyword</li> <li>SiLU activation follows the torch.nn.SiLU in the PyTorch library.</li> </ul>"},{"location":"models/layers/activations/#hardswish","title":"Hardswish","text":"<ul> <li>This can be applied by giving <code>'hard_swish'</code> keyword</li> <li>Hardswish activation follows the torch.nn.Hardswish in the PyTorch library.</li> </ul>"},{"location":"models/layers/normalizations/","title":"Normalizations","text":"<p>Normalization layers significantly affect model performance by transforming features to similar scales. To enable diverse model designs, NetsPresso Trainer supports various activation modules based on the pytorch library.</p>"},{"location":"models/layers/normalizations/#supporting-normalization-layers","title":"Supporting normalization layers","text":"<p>The currently supported normalization layers in NetsPresso Trainer are as follows.</p>"},{"location":"models/layers/normalizations/#batchnorm","title":"BatchNorm","text":"<ul> <li>This can be applied by giving <code>'batch_norm'</code> keyword</li> <li>Batch normalization follows torch.nn.BatchNorm2d in the PyTorch library.</li> </ul>"},{"location":"models/layers/normalizations/#instancenorm","title":"InstanceNorm","text":"<ul> <li>This can be applied by giving <code>'instance_norm'</code> keyword</li> <li>Instance normalization follows the torch.nn.InstanceNorm2d in the PyTorch library</li> </ul>"},{"location":"models/necks/fpn/","title":"FPN","text":"<p>FPN based on Feature Pyramid Networks for Object Detection</p> <p>The Feature Pyramid Network (FPN) is designed to enhance feature maps given from the backbone, typically used for detection models. Therefore, we also recommend to use it in detection task as well. FPN can create more pyramid deeply than the input feature pyramid from backbone, and in such cases, additional convolution or pooling layers are added.</p>"},{"location":"models/necks/fpn/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"fpn\" to use <code>FPN</code> neck. <code>params.num_outs</code> (int) The number of output feature maps. This must greater than or equal to the number of input feature maps. If <code>end_level</code> is not the last feature map produced by the backbone, extra levels that generated by setting <code>num_outs</code> beyond the number of input feature maps are not allowed. <code>params.start_level</code> (int) Determines the starting index from the list of feature maps produced by the backbone. It defines the number of input feature maps with <code>end_level</code>. <code>params.end_level</code> (int) Determines the end index from the list of feature maps produced by the backbone.  If -1, it equals to using up to the last feature map. it defines the number of input feature maps with <code>start_level</code>. <code>params.add_extra_convs</code> (str) Defines additional convolution layers for pyramid construction when <code>num_outs</code> is greater than the number of input feature maps. Options are <code>on_input</code>, <code>on_lateral</code>, <code>on_output</code>. If <code>None</code>, max pooling is applied. <code>params.relu_before_extra_convs</code> (bool) Determines whether to apply the <code>relu</code> activation function to the extra convolutions that are generated."},{"location":"models/necks/fpn/#model-configuration-examples","title":"Model configuration examples","text":"FPN (4-stage -&gt; 4-stage) <pre><code>model:\n  architecture:\n    neck:\n      name: fpn\n      params:\n        num_outs: 4\n        start_level: 0\n        end_level: -1\n        add_extra_convs: False\n        relu_before_extra_convs: False\n</code></pre>"},{"location":"models/necks/fpn/#related-links","title":"Related links","text":""},{"location":"models/necks/rtdetrhybridencoder/","title":"RT-DETR Hybrid Encoder","text":"<p>RT-DETR Hybrid Encoder based on RT-DETR: DETRs Beat YOLOs on Real-time Object Detection</p>"},{"location":"models/necks/rtdetrhybridencoder/#field-lists","title":"Field lists","text":"Field  Description <code>name</code> (str) Name must be \"rtdetr_hybrid_encoder\" to use RT-DETR Hybrid Encoder. <code>params.hidden_dim</code> (int) Hidden dimension size, default is 256 according to paper's Appendix Table A <code>params.use_encoder_idx</code> (list) Index indicating which feature level to apply encoder. Default is [2] since paper's Section 4.2 mentions AIFI only performed on S5 (highest level) <code>params.num_encoder_layers</code> (int) Number of encoder layers. <code>params.pe_temperature</code> (float) Temperature for positional encoding <code>params.num_attention_heads</code> (int) Number of attention heads. <code>params.dim_feedforward</code> (int) Dimension of feedforward network. <code>params.dropout</code> (float) Dropout rate, default is 0.0 according to configuration <code>params.attn_act_type</code> (str) Activation function type for attention, using GELU <code>params.expansion</code> (float) Expansion ratio for RepBlock in CCFF module, default is 0.5 <code>params.depth_mult</code> (float) Depth multiplier for scaling. <code>params.conv_act_type</code> (str) Activation function type for convolution layers, using SiLU according to paper's Figure 4."},{"location":"models/necks/rtdetrhybridencoder/#model-configuration-examples","title":"Model configuration examples","text":"RT-DETR Hybrid Encoder <pre><code>model:\n  architecture:\n    neck:\n      name: fpn\n      params:\n        num_outs: 4\n        start_level: 0\n        end_level: -1\n        add_extra_convs: False\n        relu_before_extra_convs: False\n</code></pre>"},{"location":"models/necks/rtdetrhybridencoder/#related-links","title":"Related links","text":""},{"location":"models/necks/yolopafpn/","title":"YOLOPAFPN","text":"<p>YOLOPAFPN based on YOLOX: Exceeding YOLO Series in 2021.</p> <p>YOLOPAFPN is a modified PAFPN for YOLOX model. Therefore, although YOLOPAFP is compatible with various backbones, we recommend to use it when constructing YOLOX models. The size is determined by <code>dep_mul</code> value, which defines the repetition of CSPLayers.</p>"},{"location":"models/necks/yolopafpn/#field-list","title":"Field list","text":"Field  Description <code>name</code> (str) Name must be \"yolopafpn\" to use <code>YOLOPAFPN</code> neck. <code>params.depthwise</code> (bool) Whether to enable depthwise convolution for the <code>YOLOPAFPN</code> neck. <code>params.dep_mul</code> (int) Multiplying factor determining the repetition count of <code>CSPLayer</code> in the backbone. <code>params.act_type</code> (int) Type of activation function for the model. Supporting activation functions are described in [here]."},{"location":"models/necks/yolopafpn/#model-configuration-examples","title":"Model configuration examples","text":"PAFPN for YOLOX-nano <pre><code>model:\n  architecture:\n    neck:\n      name: yolopafpn\n      params:\n        depthwise: True\n        dep_mul: 0.33\n        act_type: \"silu\"\n</code></pre> PAFPN for YOLOX-s <pre><code>model:\n  architecture:\n    neck:\n      name: yolopafpn\n      params:\n        depthwise: False\n        dep_mul: 0.33\n        act_type: \"silu\"\n</code></pre>"},{"location":"models/necks/yolopafpn/#related-links","title":"Related links","text":"<ul> <li><code>Megvii-BaseDetection/YOLOX</code></li> </ul>"}]}